{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import copy\n",
    "import time\n",
    "from torch.nn.modules.batchnorm import BatchNorm1d\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Stucture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0., max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_model, 2) *\n",
    "                             -(np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_dim // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_dim\n",
    "        ), \"Embedding dimension needs to be divisible by heads\"\n",
    "\n",
    "        self.values_linear = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys_linear = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries_linear = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_dim)\n",
    "\n",
    "    def forward(self, values, keys, query):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values_linear(values)\n",
    "        keys = self.keys_linear(keys)\n",
    "        queries = self.queries_linear(query)\n",
    "\n",
    "        # Get the dot product between queries and keys, and then apply the softmax\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        attention = torch.softmax(energy / (self.embed_dim ** (1 / 2)), dim=3)\n",
    "\n",
    "        # Apply the self attention to the values\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class TimeSeriesModel(nn.Module):\n",
    "    def __init__(self, embed_dim, heads, max_length,output_dim=1):\n",
    "        super().__init__()\n",
    "        self.self_attention = SelfAttention(embed_dim, heads)\n",
    "        self.global_pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        self.linear = nn.Linear(embed_dim, output_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim,max_length)\n",
    "        self.batch_norm1 = BatchNorm1d(embed_dim,max_length)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.batch_norm1(x.transpose(1, 2)).transpose(1, 2)\n",
    "        x = self.self_attention(x, x, x)  # Apply self-attention\n",
    "        x = self.batch_norm1(x.transpose(1, 2)).transpose(1, 2)\n",
    "        x = x.permute(0, 2, 1)  # Switch dimensions for pooling\n",
    "        x = self.global_pooling(x).squeeze(2)  # Apply global pooling\n",
    "        x = self.linear(x)  # Apply final linear layer\n",
    "        x = x.squeeze()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertTextToNumpyArray(text): # text array -> numpy array '[1,2,3]' -> [1,2,3]\n",
    "    # Step 1: Remove the square brackets and extra whitespace\n",
    "    text_cleaned = text.strip('[]').strip()\n",
    "\n",
    "    # Step 2: Split the cleaned text by whitespace to get individual integers as strings\n",
    "    int_strings = text_cleaned.split()\n",
    "\n",
    "\n",
    "    # Step 3: Convert the strings to integers\n",
    "    integers = [int(x) for x in int_strings]\n",
    "    integers\n",
    "\n",
    "    numpy_array = np.array(integers)\n",
    "    return numpy_array\n",
    "\n",
    "def ConvertRowToNumpy(a): # Row with last element is text array -> array with concatenated way [1,4,6,'[1,2,3]'] -> [1,4,6,1,2,3]\n",
    "    temp = ConvertTextToNumpyArray(a[8])\n",
    "    ResultedNumpy = np.concatenate((a[0:8],temp),axis=0)\n",
    "    return ResultedNumpy\n",
    "\n",
    "\n",
    "def ConvertDataFrameToNumpy(t): # Dataframe -> Numpy array\n",
    "\n",
    "    # Convert DataFrame To Numpy\n",
    "    original_array = t.values\n",
    "\n",
    "    # Use np.vectorize to apply the custom function to each row in the original array\n",
    "    vectorized_process = np.vectorize(ConvertRowToNumpy, signature='(m)->(n)')\n",
    "\n",
    "    # Process the entire original array without using an explicit for loop\n",
    "    final_array = vectorized_process(original_array)\n",
    "    return final_array\n",
    "\n",
    "def ConvertCSVToNumpy(CSVPath,p): # CSV Path -> Numpy array\n",
    "    if os.path.isfile(CSVPath):\n",
    "        t = pd.read_csv(CSVPath)\n",
    "        NumpyResult = ConvertDataFrameToNumpy(t)\n",
    "        return NumpyResult\n",
    "    else:\n",
    "        np.zeros(shape=(p,))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    sequences_padded = pad_sequence([seq for seq in sequences], batch_first=True)\n",
    "    labels = torch.as_tensor(labels)\n",
    "    return sequences_padded.float(), labels.float()        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load File name and Y data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>file name</th>\n",
       "      <th>initial temperature</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Catalyst #1: trig93</th>\n",
       "      <th>Catalyst #2: trig524 (2%)</th>\n",
       "      <th>Accelerator: cob 6% (max 0.7%)</th>\n",
       "      <th>Quality score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-05-05 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230505085243583.mp4</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>60</td>\n",
       "      <td>420</td>\n",
       "      <td>156</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-05-05 00:00:00</td>\n",
       "      <td>192.168.1.64_02_202305050100543584.mp4</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>90</td>\n",
       "      <td>390</td>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-05-30 00:00:00</td>\n",
       "      <td>192.168.1.64_02_2023053008542479.mp4</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>90</td>\n",
       "      <td>390</td>\n",
       "      <td>150</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-06-21 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230621074514667.mp4</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>90</td>\n",
       "      <td>390</td>\n",
       "      <td>150</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-06-28 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230628102913672.mp4</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>180</td>\n",
       "      <td>300</td>\n",
       "      <td>132</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-06-28 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230628090938483.mp4</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>120</td>\n",
       "      <td>360</td>\n",
       "      <td>144</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29/06/2023</td>\n",
       "      <td>192.168.1.64_02_20230629110733693.mp4</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>210</td>\n",
       "      <td>270</td>\n",
       "      <td>126</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-06-30 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230630141655536.mp4</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>240</td>\n",
       "      <td>240</td>\n",
       "      <td>120</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-06-30 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230630125757199.mp4</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>240</td>\n",
       "      <td>240</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-06-30 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230630110342551.mp4</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>210</td>\n",
       "      <td>270</td>\n",
       "      <td>126</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-06-30 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230630075413606.mp4</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>150</td>\n",
       "      <td>330</td>\n",
       "      <td>138</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-06-30 00:00:00</td>\n",
       "      <td>192.168.1.64_02_2023063009123691.mp4</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>180</td>\n",
       "      <td>300</td>\n",
       "      <td>132</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-07-03 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230703152811998.mp4</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>270</td>\n",
       "      <td>210</td>\n",
       "      <td>114</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-07-03 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230703105804137.mp4</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>180</td>\n",
       "      <td>300</td>\n",
       "      <td>132</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023-07-03 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230703081328296.mp4</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>180</td>\n",
       "      <td>300</td>\n",
       "      <td>132</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2023-07-04 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230704140039400.mp4</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>270</td>\n",
       "      <td>210</td>\n",
       "      <td>114</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2023-07-04 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230704123206846.mp4</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>240</td>\n",
       "      <td>240</td>\n",
       "      <td>120</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2023-07-04 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230704085041176.mp4</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>150</td>\n",
       "      <td>330</td>\n",
       "      <td>138</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023-07-04 00:00:00</td>\n",
       "      <td>192.168.1.64_02_2023070409513072.mp4</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>180</td>\n",
       "      <td>300</td>\n",
       "      <td>132</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023-07-05 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230705132156506.mp4</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>270</td>\n",
       "      <td>210</td>\n",
       "      <td>114</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2023-07-05 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230705081807192.mp4</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>150</td>\n",
       "      <td>330</td>\n",
       "      <td>138</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2023-07-06 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230706140019500.mp4</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>300</td>\n",
       "      <td>180</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2023-07-06 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230706104339819.mp4</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>210</td>\n",
       "      <td>270</td>\n",
       "      <td>126</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2023-07-06 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230706092812792.mp4</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>180</td>\n",
       "      <td>300</td>\n",
       "      <td>132</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2023-07-06 00:00:00</td>\n",
       "      <td>192.168.1.64_02_202307060757318.mp4</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>150</td>\n",
       "      <td>330</td>\n",
       "      <td>138</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2023-07-07 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230707144338764.mp4</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>300</td>\n",
       "      <td>180</td>\n",
       "      <td>108</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2023-07-07 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230707133707750.mp4</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>300</td>\n",
       "      <td>180</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2023-07-07 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230707113101571.mp4</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>240</td>\n",
       "      <td>240</td>\n",
       "      <td>120</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2023-07-07 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230707093029952.mp4</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>210</td>\n",
       "      <td>270</td>\n",
       "      <td>126</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2023-07-07 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230707081158669.mp4</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>180</td>\n",
       "      <td>300</td>\n",
       "      <td>132</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2023-07-10 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230710104627840.mp4</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>210</td>\n",
       "      <td>270</td>\n",
       "      <td>126</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2023-07-18 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230718103111134.mp4</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>270</td>\n",
       "      <td>210</td>\n",
       "      <td>114</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2023-07-18 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230718093841712.mp4</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>240</td>\n",
       "      <td>240</td>\n",
       "      <td>120</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2023-07-18 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230718085116872.mp4</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>240</td>\n",
       "      <td>240</td>\n",
       "      <td>120</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2023-07-18 00:00:00</td>\n",
       "      <td>192.168.1.64_02_2023071811284912.mp4</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>270</td>\n",
       "      <td>210</td>\n",
       "      <td>114</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2023-07-20 00:00:00</td>\n",
       "      <td>192.168.1.64_02_20230720094332694.mp4</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>240</td>\n",
       "      <td>240</td>\n",
       "      <td>120</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Date                               file name  \\\n",
       "0   2023-05-05 00:00:00   192.168.1.64_02_20230505085243583.mp4   \n",
       "1   2023-05-05 00:00:00  192.168.1.64_02_202305050100543584.mp4   \n",
       "2   2023-05-30 00:00:00    192.168.1.64_02_2023053008542479.mp4   \n",
       "3   2023-06-21 00:00:00   192.168.1.64_02_20230621074514667.mp4   \n",
       "4   2023-06-28 00:00:00   192.168.1.64_02_20230628102913672.mp4   \n",
       "5   2023-06-28 00:00:00   192.168.1.64_02_20230628090938483.mp4   \n",
       "6            29/06/2023   192.168.1.64_02_20230629110733693.mp4   \n",
       "7   2023-06-30 00:00:00   192.168.1.64_02_20230630141655536.mp4   \n",
       "8   2023-06-30 00:00:00   192.168.1.64_02_20230630125757199.mp4   \n",
       "9   2023-06-30 00:00:00   192.168.1.64_02_20230630110342551.mp4   \n",
       "10  2023-06-30 00:00:00   192.168.1.64_02_20230630075413606.mp4   \n",
       "11  2023-06-30 00:00:00    192.168.1.64_02_2023063009123691.mp4   \n",
       "12  2023-07-03 00:00:00   192.168.1.64_02_20230703152811998.mp4   \n",
       "13  2023-07-03 00:00:00   192.168.1.64_02_20230703105804137.mp4   \n",
       "14  2023-07-03 00:00:00   192.168.1.64_02_20230703081328296.mp4   \n",
       "15  2023-07-04 00:00:00   192.168.1.64_02_20230704140039400.mp4   \n",
       "16  2023-07-04 00:00:00   192.168.1.64_02_20230704123206846.mp4   \n",
       "17  2023-07-04 00:00:00   192.168.1.64_02_20230704085041176.mp4   \n",
       "18  2023-07-04 00:00:00    192.168.1.64_02_2023070409513072.mp4   \n",
       "19  2023-07-05 00:00:00   192.168.1.64_02_20230705132156506.mp4   \n",
       "20  2023-07-05 00:00:00   192.168.1.64_02_20230705081807192.mp4   \n",
       "21  2023-07-06 00:00:00   192.168.1.64_02_20230706140019500.mp4   \n",
       "22  2023-07-06 00:00:00   192.168.1.64_02_20230706104339819.mp4   \n",
       "23  2023-07-06 00:00:00   192.168.1.64_02_20230706092812792.mp4   \n",
       "24  2023-07-06 00:00:00     192.168.1.64_02_202307060757318.mp4   \n",
       "25  2023-07-07 00:00:00   192.168.1.64_02_20230707144338764.mp4   \n",
       "26  2023-07-07 00:00:00   192.168.1.64_02_20230707133707750.mp4   \n",
       "27  2023-07-07 00:00:00   192.168.1.64_02_20230707113101571.mp4   \n",
       "28  2023-07-07 00:00:00   192.168.1.64_02_20230707093029952.mp4   \n",
       "29  2023-07-07 00:00:00   192.168.1.64_02_20230707081158669.mp4   \n",
       "30  2023-07-10 00:00:00   192.168.1.64_02_20230710104627840.mp4   \n",
       "31  2023-07-18 00:00:00   192.168.1.64_02_20230718103111134.mp4   \n",
       "32  2023-07-18 00:00:00   192.168.1.64_02_20230718093841712.mp4   \n",
       "33  2023-07-18 00:00:00   192.168.1.64_02_20230718085116872.mp4   \n",
       "34  2023-07-18 00:00:00    192.168.1.64_02_2023071811284912.mp4   \n",
       "35  2023-07-20 00:00:00   192.168.1.64_02_20230720094332694.mp4   \n",
       "\n",
       "    initial temperature  Unnamed: 3  Catalyst #1: trig93  \\\n",
       "0                    18          18                   60   \n",
       "1                    20          20                   90   \n",
       "2                    19          20                   90   \n",
       "3                    20          20                   90   \n",
       "4                    25          26                  180   \n",
       "5                    22          22                  120   \n",
       "6                    27          28                  210   \n",
       "7                    30          30                  240   \n",
       "8                    30          30                  240   \n",
       "9                    27          28                  210   \n",
       "10                   23          24                  150   \n",
       "11                   25          26                  180   \n",
       "12                   32          32                  270   \n",
       "13                   26          26                  180   \n",
       "14                   25          26                  180   \n",
       "15                   32          32                  270   \n",
       "16                   30          30                  240   \n",
       "17                   24          24                  150   \n",
       "18                   26          26                  180   \n",
       "19                   32          32                  270   \n",
       "20                   23          24                  150   \n",
       "21                   33          34                  300   \n",
       "22                   28          28                  210   \n",
       "23                   26          26                  180   \n",
       "24                   24          24                  150   \n",
       "25                   34          34                  300   \n",
       "26                   33          34                  300   \n",
       "27                   29          30                  240   \n",
       "28                   27          28                  210   \n",
       "29                   26          26                  180   \n",
       "30                   28          28                  210   \n",
       "31                   31          32                  270   \n",
       "32                   30          30                  240   \n",
       "33                   30          30                  240   \n",
       "34                   32          32                  270   \n",
       "35                   30          30                  240   \n",
       "\n",
       "    Catalyst #2: trig524 (2%)  Accelerator: cob 6% (max 0.7%)  Quality score  \n",
       "0                         420                             156              7  \n",
       "1                         390                             150              5  \n",
       "2                         390                             150              6  \n",
       "3                         390                             150              8  \n",
       "4                         300                             132              3  \n",
       "5                         360                             144              4  \n",
       "6                         270                             126              6  \n",
       "7                         240                             120              7  \n",
       "8                         240                             120              3  \n",
       "9                         270                             126              2  \n",
       "10                        330                             138              7  \n",
       "11                        300                             132              6  \n",
       "12                        210                             114              5  \n",
       "13                        300                             132              5  \n",
       "14                        300                             132              4  \n",
       "15                        210                             114              5  \n",
       "16                        240                             120              4  \n",
       "17                        330                             138              6  \n",
       "18                        300                             132              7  \n",
       "19                        210                             114              4  \n",
       "20                        330                             138              5  \n",
       "21                        180                             108              5  \n",
       "22                        270                             126              6  \n",
       "23                        300                             132              7  \n",
       "24                        330                             138              3  \n",
       "25                        180                             108              4  \n",
       "26                        180                             108              5  \n",
       "27                        240                             120              6  \n",
       "28                        270                             126              6  \n",
       "29                        300                             132              7  \n",
       "30                        270                             126              3  \n",
       "31                        210                             114              6  \n",
       "32                        240                             120              7  \n",
       "33                        240                             120              7  \n",
       "34                        210                             114              5  \n",
       "35                        240                             120              4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RootPath = \"D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\"\n",
    "PathLoadExcel = os.path.join(RootPath,\"VirtFuse data 26072023.xlsx\")\n",
    "t = pd.read_excel(PathLoadExcel)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ 4475 21834 20889 94056 42856  5078  1331   999   393    89]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.values[0][8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Time Series Feature From the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230505085243583.csv': 7,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_2023053008542479.csv': 6,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230628102913672.csv': 3,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230628090938483.csv': 4,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230629110733693.csv': 6,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230630141655536.csv': 7,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230630125757199.csv': 3,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230630110342551.csv': 2,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230630075413606.csv': 7,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_2023063009123691.csv': 6,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230703152811998.csv': 5,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230703105804137.csv': 5,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230703081328296.csv': 4,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230704140039400.csv': 5,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230704123206846.csv': 4,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230704085041176.csv': 6,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_2023070409513072.csv': 7,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230705132156506.csv': 4,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230705081807192.csv': 5,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230706140019500.csv': 5,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230706104339819.csv': 6,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230706092812792.csv': 7,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_202307060757318.csv': 3,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230707144338764.csv': 4,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230707133707750.csv': 5,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230707113101571.csv': 6,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230707093029952.csv': 6,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230707081158669.csv': 7,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230710104627840.csv': 3,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230718103111134.csv': 6,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230718093841712.csv': 7,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230718085116872.csv': 7,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_2023071811284912.csv': 5,\n",
       " 'D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\\\\192.168.1.64_02_20230720094332694.csv': 4}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RootPath = \"D:\\\\DoctoralSharif\\\\NoranCompany\\\\Proposal\\\\KyKlos\\\\Data\\\\DataForAIModel\"\n",
    "PathLoadExcel = os.path.join(RootPath,\"VirtFuse data 26072023.xlsx\")\n",
    "\n",
    "\n",
    "t = pd.read_excel(PathLoadExcel)\n",
    "# print(t)\n",
    "type(t['file name'].values)\n",
    "\n",
    "text = t['file name'].values[10]\n",
    "# print(text)\n",
    "def ExtractCSVName(text):\n",
    "    ## Extract The file name\n",
    "    temp = text.split(\".\")[0:-1]\n",
    "    return '.'.join(temp) + '.csv'\n",
    "\n",
    "text = ExtractCSVName(text)    \n",
    "# print(text)\n",
    "filePath = os.path.join(RootPath,text)\n",
    "\n",
    "\n",
    "def ExtaractFilePath(RootPath,text):\n",
    "    text = ExtractCSVName(text)    \n",
    "    filePath = os.path.join(RootPath,text)\n",
    "    return filePath\n",
    "\n",
    "\n",
    "ExtaractFilePath(RootPath,text)\n",
    "\n",
    "filePath = np.vectorize(ExtaractFilePath, excluded=[RootPath])\n",
    "\n",
    "FilePathArray = filePath(RootPath=RootPath,text = t['file name'].values)\n",
    "\n",
    "\n",
    "## Check Number OF Missing\n",
    "def isFile(filePath):\n",
    "    return os.path.isfile(filePath)\n",
    "CheckFile = np.vectorize(isFile)\n",
    "index = np.where(CheckFile(FilePathArray))\n",
    "FilePathArray = FilePathArray[index]     \n",
    "a = zip(FilePathArray,t['Quality score'].values[index])\n",
    "dictionary = {k:v for k,v in a}\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Filtering the non-existence files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "       19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Check(index):\n",
    "    text = t['file name'].values[index]\n",
    "    temp = text.split(\".\")[0:-1]\n",
    "    R = '.'.join(temp) + '.csv'\n",
    "    return dictionary[os.path.join(RootPath,R)] == t['Quality score'].values[index]\n",
    "\n",
    "T = np.vectorize(Check)\n",
    "\n",
    "t1 = np.linspace(0,FilePathArray.shape[0]-1,FilePathArray.shape[0]).astype(int)\n",
    "n = [t1[0],t1[2],t1[4]]\n",
    "n = np.array(n, dtype=int)\n",
    "n = np.concatenate((n,t1[5:]),axis=0,dtype=int)\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extraction Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractX(CSVPath,p):\n",
    "    X = ConvertCSVToNumpy(CSVPath,p)\n",
    "    return X\n",
    "    \n",
    "\n",
    "VectorizedX = np.vectorize(ExtractX,excluded=['p'])    \n",
    "InputFeature  = VectorizedX(CSVPath = FilePathArray,p=18)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30195, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InputFeature[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert To Tensor Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "XData = [torch.tensor(InputFeature[i][30:1030].astype(float)) for i in np.arange(InputFeature.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "YData = torch.tensor(t['Quality score'].values[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Tensor Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableLengthSequenceDataset(Dataset):\n",
    "    def __init__(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data[idx], self.target[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VariableLengthSequenceDataset(XData,YData)\n",
    "# dataloader = DataLoader(dataset, batch_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Division train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_length = len(dataset)\n",
    "# train_sets, val_sets, test_sets = torch.utils.data.random_split(dataset, [np.floor(data_length*(1-test_per-val_per)).astype(int),np.floor(data_length*(val_per)).astype(int), np.ceil(data_length*(test_per)).astype(int)])\n",
    "train_sets, test_sets = torch.utils.data.random_split(dataset, [int(np.ceil(0.8*data_length)), int(np.floor(0.2*data_length))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_sets)\n",
    "dataloaders = {}\n",
    "batch_size = 3\n",
    "\n",
    "dataloaders['train'] = DataLoader(train_sets, batch_size=batch_size, collate_fn=collate_fn,\n",
    "                                              shuffle=True) \n",
    "\n",
    "dataloaders['test'] = DataLoader(test_sets, batch_size=batch_size, collate_fn=collate_fn,\n",
    "                                              shuffle=True)\n",
    "\n",
    "\n",
    "dataset_sizes = {}\n",
    "\n",
    "dataset_sizes[\"train\"] = len(train_sets)\n",
    "dataset_sizes[\"test\"]  = len(test_sets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1000, 18])\n",
      "torch.Size([3])\n",
      "torch.Size([3, 1000, 18])\n",
      "torch.Size([3])\n",
      "torch.Size([3, 1000, 18])\n",
      "torch.Size([3])\n",
      "torch.Size([3, 1000, 18])\n",
      "torch.Size([3])\n",
      "torch.Size([3, 1000, 18])\n",
      "torch.Size([3])\n",
      "torch.Size([3, 1000, 18])\n",
      "torch.Size([3])\n",
      "torch.Size([3, 1000, 18])\n",
      "torch.Size([3])\n",
      "torch.Size([3, 1000, 18])\n",
      "torch.Size([3])\n",
      "torch.Size([3, 1000, 18])\n",
      "torch.Size([3])\n",
      "torch.Size([1, 1000, 18])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for i, (data, target) in enumerate(dataloaders[\"train\"]):\n",
    "    print(data.size())\n",
    "    print(target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "torch.Size([])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "SelfAttentionModel = TimeSeriesModel(18,2,5000,1)\n",
    "SelfAttentionModel.to(device)\n",
    "for i, (data, target) in enumerate(dataloaders[\"train\"]):\n",
    "    print(SelfAttentionModel(data.to(device)).size())\n",
    "    print(target.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessary Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightClipper(object):\n",
    "    def __init__(self, frequency=5):\n",
    "        self.frequency = frequency\n",
    "    def __call__(self, module):\n",
    "        # filter the variables to get the ones you want\n",
    "        if hasattr(module, 'weight'):\n",
    "            if type(module) == type(nn.Linear(512,2)):\n",
    "                w = module.weight.data\n",
    "                # b = module.bias.data\n",
    "                # b = b.clamp(-0.2,0.2)\n",
    "                # module.bias.data = b\n",
    "                w = w.clamp(-1,1)\n",
    "                module.weight.data = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model1, criterion, scheduler, optimizer, dataloaders,dataset_sizes,lambda2,constrain,num_epochs=25,initial_best_Lossuracy=0):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model1_wts = copy.deepcopy(model1.state_dict())\n",
    "    best_Loss = initial_best_Lossuracy\n",
    "    sum_epoch_acc = 0.0\n",
    "    sum_square_acc = 0.0\n",
    "    test_loss_epoch = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and testidation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model1.train()  # Set model1 to training mode\n",
    "            else:\n",
    "                model1.eval()   # Set model1 to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model1(inputs.float())\n",
    "                    # print(outputs.size())\n",
    "                    # print(labels.size())\n",
    "                    # _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        l2_reg = torch.tensor(0.).to(device).float()\n",
    "                        for param in model1.parameters():\n",
    "                            if param.requires_grad == True:\n",
    "                                l2_reg += torch.norm(param)\n",
    "                        loss = criterion(outputs, labels.float()) + lambda2 * l2_reg\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        model1.apply(constrain)\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                # running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                epoch_loss_train = running_loss / dataset_sizes[phase]\n",
    "                # epoch_acc_train = running_corrects.double() / dataset_sizes[phase]\n",
    "                print('{} Loss: {:.4f}'.format(\n",
    "                    phase, epoch_loss_train))\n",
    "            if phase == 'test':\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                # running_corrects.double() / dataset_sizes[phase]\n",
    "                # sum_epoch_acc += epoch_acc\n",
    "                # sum_square_acc += epoch_acc ** 2\n",
    "                test_loss_epoch.append(epoch_loss)\n",
    "                print('{} Loss: {:.4f}'.format(\n",
    "                    phase, epoch_loss))\n",
    "            # deep copy the model1\n",
    "            if phase == 'test' and epoch_loss < best_Loss:\n",
    "                best_Loss = epoch_loss\n",
    "                best_Loss_train = epoch_loss_train \n",
    "                best_model1_wts = copy.deepcopy(model1.state_dict())\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Loss: {:4f}'.format(best_Loss))\n",
    "\n",
    "    # load best model1 weights\n",
    "    model1.load_state_dict(best_model1_wts)\n",
    "    return model1,best_Loss,test_loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "SelfAttentionModel = TimeSeriesModel(18,2,1)\n",
    "SelfAttentionModel.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer_ft = torch.optim.Adam(SelfAttentionModel.parameters(),lr=0.0003)\n",
    "optimizer_ft =  torch.optim.SGD(SelfAttentionModel.parameters(), lr=0.003, momentum=0.9)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "constrain = WeightClipper()\n",
    "model,best_Loss,test_loss_epoch = train_model(SelfAttentionModel,criterion,exp_lr_scheduler,optimizer_ft,dataloaders,0.004,constrain,50,torch.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Loss_{}.pt'.format(best_Loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m i, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloaders[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[0;32m      2\u001b[0m     out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39mto(device))\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(out)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataloaders' is not defined"
     ]
    }
   ],
   "source": [
    "for i, (data, target) in enumerate(dataloaders[\"test\"]):\n",
    "    out = model(data.to(device))\n",
    "    print(out)\n",
    "    print(target)\n",
    "    print(criterion(target.to(device),out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 24.7864\n",
      "test Loss: 15.9385\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 15.0094\n",
      "test Loss: 5.9081\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 4.7848\n",
      "test Loss: 3.1561\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 3.3772\n",
      "test Loss: 7.8398\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 3.0432\n",
      "test Loss: 4.4392\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 1.1431\n",
      "test Loss: 2.6870\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 1.6404\n",
      "test Loss: 2.5225\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 1.8151\n",
      "test Loss: 2.5434\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 1.7668\n",
      "test Loss: 2.5887\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 1.6891\n",
      "test Loss: 2.6524\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 1.6049\n",
      "test Loss: 2.7298\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 1.5269\n",
      "test Loss: 2.8158\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 1.4611\n",
      "test Loss: 2.9055\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 1.4089\n",
      "test Loss: 2.9942\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 1.3628\n",
      "test Loss: 3.0023\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 1.3594\n",
      "test Loss: 3.0098\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 1.3564\n",
      "test Loss: 3.0169\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 1.3537\n",
      "test Loss: 3.0237\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 1.3511\n",
      "test Loss: 3.0302\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 1.3486\n",
      "test Loss: 3.0367\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 1.3462\n",
      "test Loss: 3.0430\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 1.3434\n",
      "test Loss: 3.0436\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 1.3431\n",
      "test Loss: 3.0442\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 1.3429\n",
      "test Loss: 3.0449\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 1.3427\n",
      "test Loss: 3.0455\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 1.3425\n",
      "test Loss: 3.0461\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 1.3423\n",
      "test Loss: 3.0467\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 1.3421\n",
      "test Loss: 3.0473\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 1.3418\n",
      "test Loss: 3.0473\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 1.3418\n",
      "test Loss: 3.0474\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 1.3417\n",
      "test Loss: 3.0474\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 1.3417\n",
      "test Loss: 3.0475\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 1.3417\n",
      "test Loss: 3.0476\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 1.3417\n",
      "test Loss: 3.0476\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 1.3417\n",
      "test Loss: 3.0477\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 1.3416\n",
      "test Loss: 3.0477\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 1.3416\n",
      "test Loss: 3.0477\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 1.3416\n",
      "test Loss: 3.0477\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 1.3416\n",
      "test Loss: 3.0477\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 1.3416\n",
      "test Loss: 3.0477\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 1.3416\n",
      "test Loss: 3.0477\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 1.3416\n",
      "test Loss: 3.0477\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 1.3416\n",
      "test Loss: 3.0477\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 1.3416\n",
      "test Loss: 3.0477\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 1.3416\n",
      "test Loss: 3.0477\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 1.3416\n",
      "test Loss: 3.0477\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 1.3416\n",
      "test Loss: 3.0477\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 1.3416\n",
      "test Loss: 3.0477\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 1.3416\n",
      "test Loss: 3.0477\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 1.3416\n",
      "test Loss: 3.0477\n",
      "Training complete in 0m 7s\n",
      "Best val Loss: 2.522526\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 24.4474\n",
      "test Loss: 25.5920\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 14.1393\n",
      "test Loss: 16.3448\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 4.1298\n",
      "test Loss: 7.7809\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 3.9830\n",
      "test Loss: 4.2113\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 2.3799\n",
      "test Loss: 4.1058\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 1.2225\n",
      "test Loss: 3.5716\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 1.9390\n",
      "test Loss: 3.1924\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 1.7782\n",
      "test Loss: 3.1753\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 1.6948\n",
      "test Loss: 3.1667\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 1.5942\n",
      "test Loss: 3.1609\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 1.4994\n",
      "test Loss: 3.1552\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 1.4210\n",
      "test Loss: 3.1485\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 1.3617\n",
      "test Loss: 3.1407\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 1.3195\n",
      "test Loss: 3.1322\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 1.2807\n",
      "test Loss: 3.1313\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 1.2785\n",
      "test Loss: 3.1305\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 1.2765\n",
      "test Loss: 3.1297\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 1.2749\n",
      "test Loss: 3.1289\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 1.2733\n",
      "test Loss: 3.1282\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 1.2719\n",
      "test Loss: 3.1274\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 1.2706\n",
      "test Loss: 3.1267\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 1.2686\n",
      "test Loss: 3.1266\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 1.2685\n",
      "test Loss: 3.1266\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 1.2683\n",
      "test Loss: 3.1265\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 1.2682\n",
      "test Loss: 3.1264\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 1.2681\n",
      "test Loss: 3.1263\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 1.2680\n",
      "test Loss: 3.1263\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 1.2679\n",
      "test Loss: 3.1262\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 1.2677\n",
      "test Loss: 3.1262\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 1.2677\n",
      "test Loss: 3.1262\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 1.2676\n",
      "test Loss: 3.1262\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 1.2676\n",
      "test Loss: 3.1262\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 1.2676\n",
      "test Loss: 3.1262\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 1.2676\n",
      "test Loss: 3.1262\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 1.2676\n",
      "test Loss: 3.1261\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 1.2676\n",
      "test Loss: 3.1261\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 1.2676\n",
      "test Loss: 3.1261\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 1.2676\n",
      "test Loss: 3.1261\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 1.2676\n",
      "test Loss: 3.1261\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 1.2676\n",
      "test Loss: 3.1261\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 1.2676\n",
      "test Loss: 3.1261\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 1.2676\n",
      "test Loss: 3.1261\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 1.2676\n",
      "test Loss: 3.1261\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 1.2676\n",
      "test Loss: 3.1261\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 1.2676\n",
      "test Loss: 3.1261\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 1.2676\n",
      "test Loss: 3.1261\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 1.2676\n",
      "test Loss: 3.1261\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 1.2676\n",
      "test Loss: 3.1261\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 1.2676\n",
      "test Loss: 3.1261\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 1.2676\n",
      "test Loss: 3.1261\n",
      "Training complete in 0m 8s\n",
      "Best val Loss: 3.126140\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 27.2088\n",
      "test Loss: 16.2942\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 15.7294\n",
      "test Loss: 6.0169\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 4.3223\n",
      "test Loss: 2.1029\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 4.3840\n",
      "test Loss: 3.8521\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 2.3429\n",
      "test Loss: 1.9443\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 1.2495\n",
      "test Loss: 2.1187\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 2.0202\n",
      "test Loss: 2.0721\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 1.8032\n",
      "test Loss: 2.0499\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 1.7116\n",
      "test Loss: 2.0226\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 1.6029\n",
      "test Loss: 1.9996\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 1.5004\n",
      "test Loss: 1.9852\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 1.4150\n",
      "test Loss: 1.9798\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 1.3495\n",
      "test Loss: 1.9818\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 1.3025\n",
      "test Loss: 1.9889\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 1.2617\n",
      "test Loss: 1.9898\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 1.2591\n",
      "test Loss: 1.9906\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 1.2569\n",
      "test Loss: 1.9913\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 1.2550\n",
      "test Loss: 1.9920\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 1.2533\n",
      "test Loss: 1.9927\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 1.2516\n",
      "test Loss: 1.9934\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 1.2501\n",
      "test Loss: 1.9941\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 1.2480\n",
      "test Loss: 1.9941\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 1.2478\n",
      "test Loss: 1.9942\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 1.2477\n",
      "test Loss: 1.9943\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 1.2476\n",
      "test Loss: 1.9943\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 1.2474\n",
      "test Loss: 1.9944\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 1.2473\n",
      "test Loss: 1.9945\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 1.2472\n",
      "test Loss: 1.9945\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 1.2470\n",
      "test Loss: 1.9945\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 1.2469\n",
      "test Loss: 1.9945\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 1.2469\n",
      "test Loss: 1.9945\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 1.2469\n",
      "test Loss: 1.9945\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 1.2469\n",
      "test Loss: 1.9945\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 1.2469\n",
      "test Loss: 1.9946\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 1.2469\n",
      "test Loss: 1.9946\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 1.2469\n",
      "test Loss: 1.9946\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 1.2469\n",
      "test Loss: 1.9946\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 1.2469\n",
      "test Loss: 1.9946\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 1.2469\n",
      "test Loss: 1.9946\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 1.2469\n",
      "test Loss: 1.9946\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 1.2469\n",
      "test Loss: 1.9946\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 1.2469\n",
      "test Loss: 1.9946\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 1.2468\n",
      "test Loss: 1.9946\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 1.2468\n",
      "test Loss: 1.9946\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 1.2468\n",
      "test Loss: 1.9946\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 1.2468\n",
      "test Loss: 1.9946\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 1.2468\n",
      "test Loss: 1.9946\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 1.2468\n",
      "test Loss: 1.9946\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 1.2468\n",
      "test Loss: 1.9946\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 1.2468\n",
      "test Loss: 1.9946\n",
      "Training complete in 0m 4s\n",
      "Best val Loss: 1.944284\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 26.6299\n",
      "test Loss: 17.4786\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 15.1734\n",
      "test Loss: 7.0433\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 4.1068\n",
      "test Loss: 2.7942\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 4.4726\n",
      "test Loss: 3.8874\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 2.1444\n",
      "test Loss: 2.4594\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 1.3614\n",
      "test Loss: 2.8231\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 2.1063\n",
      "test Loss: 2.7143\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 1.8093\n",
      "test Loss: 2.6761\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 1.7168\n",
      "test Loss: 2.6293\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 1.6117\n",
      "test Loss: 2.5849\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 1.5153\n",
      "test Loss: 2.5480\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 1.4368\n",
      "test Loss: 2.5204\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 1.3780\n",
      "test Loss: 2.5014\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 1.3366\n",
      "test Loss: 2.4893\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 1.2987\n",
      "test Loss: 2.4884\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 1.2965\n",
      "test Loss: 2.4878\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 1.2947\n",
      "test Loss: 2.4872\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 1.2931\n",
      "test Loss: 2.4867\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 1.2916\n",
      "test Loss: 2.4863\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 1.2903\n",
      "test Loss: 2.4859\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 1.2890\n",
      "test Loss: 2.4856\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 1.2871\n",
      "test Loss: 2.4855\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 1.2870\n",
      "test Loss: 2.4855\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 1.2869\n",
      "test Loss: 2.4855\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 1.2868\n",
      "test Loss: 2.4854\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 1.2866\n",
      "test Loss: 2.4854\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 1.2865\n",
      "test Loss: 2.4854\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 1.2864\n",
      "test Loss: 2.4853\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 1.2862\n",
      "test Loss: 2.4853\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 1.2862\n",
      "test Loss: 2.4853\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 1.2862\n",
      "test Loss: 2.4853\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 1.2862\n",
      "test Loss: 2.4853\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 1.2862\n",
      "test Loss: 2.4853\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 1.2862\n",
      "test Loss: 2.4853\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 1.2862\n",
      "test Loss: 2.4853\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 1.2862\n",
      "test Loss: 2.4853\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 1.2862\n",
      "test Loss: 2.4853\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 1.2862\n",
      "test Loss: 2.4853\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 1.2862\n",
      "test Loss: 2.4853\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 1.2862\n",
      "test Loss: 2.4853\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 1.2861\n",
      "test Loss: 2.4853\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 1.2861\n",
      "test Loss: 2.4853\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 1.2861\n",
      "test Loss: 2.4853\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 1.2861\n",
      "test Loss: 2.4853\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 1.2861\n",
      "test Loss: 2.4853\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 1.2861\n",
      "test Loss: 2.4853\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 1.2861\n",
      "test Loss: 2.4853\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 1.2861\n",
      "test Loss: 2.4853\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 1.2861\n",
      "test Loss: 2.4853\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 1.2861\n",
      "test Loss: 2.4853\n",
      "Training complete in 0m 3s\n",
      "Best val Loss: 2.459441\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 25.2466\n",
      "test Loss: 9.1216\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 14.9459\n",
      "test Loss: 2.2683\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 4.4212\n",
      "test Loss: 18.5340\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 3.7418\n",
      "test Loss: 32.7761\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 2.6839\n",
      "test Loss: 11.6762\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 1.0906\n",
      "test Loss: 3.5048\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 1.7734\n",
      "test Loss: 2.8824\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 1.7972\n",
      "test Loss: 2.9939\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 1.7284\n",
      "test Loss: 3.2174\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 1.6339\n",
      "test Loss: 3.5282\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 1.5377\n",
      "test Loss: 3.9013\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 1.4528\n",
      "test Loss: 4.3092\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 1.3839\n",
      "test Loss: 4.7249\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 1.3316\n",
      "test Loss: 5.1252\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 1.2859\n",
      "test Loss: 5.1611\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 1.2827\n",
      "test Loss: 5.1933\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 1.2800\n",
      "test Loss: 5.2232\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 1.2775\n",
      "test Loss: 5.2516\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 1.2752\n",
      "test Loss: 5.2788\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 1.2730\n",
      "test Loss: 5.3053\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 1.2709\n",
      "test Loss: 5.3312\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 1.2683\n",
      "test Loss: 5.3337\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 1.2681\n",
      "test Loss: 5.3362\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 1.2679\n",
      "test Loss: 5.3386\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 1.2677\n",
      "test Loss: 5.3411\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 1.2675\n",
      "test Loss: 5.3436\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 1.2673\n",
      "test Loss: 5.3460\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 1.2671\n",
      "test Loss: 5.3484\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 1.2669\n",
      "test Loss: 5.3487\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 1.2669\n",
      "test Loss: 5.3489\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 1.2669\n",
      "test Loss: 5.3492\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 1.2668\n",
      "test Loss: 5.3494\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 1.2668\n",
      "test Loss: 5.3497\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 1.2668\n",
      "test Loss: 5.3499\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 1.2668\n",
      "test Loss: 5.3501\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 1.2668\n",
      "test Loss: 5.3502\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 1.2668\n",
      "test Loss: 5.3502\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 1.2668\n",
      "test Loss: 5.3502\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 1.2668\n",
      "test Loss: 5.3502\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 1.2667\n",
      "test Loss: 5.3502\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 1.2667\n",
      "test Loss: 5.3503\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 1.2667\n",
      "test Loss: 5.3503\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 1.2667\n",
      "test Loss: 5.3503\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 1.2667\n",
      "test Loss: 5.3503\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 1.2667\n",
      "test Loss: 5.3503\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 1.2667\n",
      "test Loss: 5.3503\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 1.2667\n",
      "test Loss: 5.3503\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 1.2667\n",
      "test Loss: 5.3503\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 1.2667\n",
      "test Loss: 5.3503\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 1.2667\n",
      "test Loss: 5.3503\n",
      "Training complete in 0m 3s\n",
      "Best val Loss: 2.268302\n"
     ]
    }
   ],
   "source": [
    "num_experiment = 5\n",
    "num_epochs = 50\n",
    "a = np.zeros(shape=(num_experiment,num_epochs))\n",
    "Bests = np.zeros(shape=(num_experiment,))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "for i in np.arange(num_experiment):\n",
    "    SelfAttentionModel = TimeSeriesModel(18,2,1)\n",
    "    SelfAttentionModel.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    # optimizer_ft = torch.optim.Adam(SelfAttentionModel.parameters(),lr=0.0003)\n",
    "    optimizer_ft =  torch.optim.SGD(SelfAttentionModel.parameters(), lr=0.005, momentum=0.9)\n",
    "    exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "    constrain = WeightClipper()\n",
    "    model,best_Loss,test_loss_epoch = train_model(SelfAttentionModel,criterion,exp_lr_scheduler,optimizer_ft,dataloaders,dataset_sizes ,0.004,constrain,50,torch.inf)\n",
    "    a[i] = np.array(test_loss_epoch)\n",
    "    Bests[i] = best_Loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.756904309514312"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Bests)\n",
    "np.std(Bests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9442841176475798"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(Bests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2008094551440864"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(a,axis=0)\n",
    "sem = scipy.stats.sem(a,axis=0)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence = 0.90\n",
    "ci = scipy.stats.t.interval(confidence, 4, loc=mean, scale=sem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.76936517  6.42289581  2.30254944  4.83501525  8.34315199  5.93662033\n",
      "  3.5743103   3.48547188  3.44998523  3.44547239  3.4586039   3.48114792\n",
      "  3.50794052  3.53574697  3.538463    3.54111192  3.54370554  3.54625163\n",
      "  3.54875418  3.55121594  3.55363965  3.55387822  3.55411461  3.55434936\n",
      "  3.55458324  3.55481649  3.55504863  3.55528066  3.55530395  3.55532726\n",
      "  3.55535047  3.55537385  3.55539698  3.55542017  3.55544384  3.55544594\n",
      "  3.55544804  3.55545027  3.55545211  3.55545457  3.55545652  3.55545875\n",
      "  3.55545875  3.55545875  3.55545875  3.55545875  3.55545875  3.55545875\n",
      "  3.55545875  3.55545875]\n"
     ]
    }
   ],
   "source": [
    "print(ci[0])\n",
    "# np.arange(1,50,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'MSE for test data')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAHwCAYAAAB332GFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAABEnUlEQVR4nO3deZhjd33n+89XW5WqVNVLVe/t7sZLMMYGB4xtQoYAgbAlwNxMAoRwTcIMN7nJBG4WQjZCeLJPIFwSMnPJQCCEkCczgcBMDIEYMEuMjQ3GeIFgm17c3e7F7qWqa5e+949z1KVWSSqVSmfR6ffrefSUdHRK59etx09//P39zvdn7i4AAABEL5f0AAAAAC4WBC8AAICYELwAAABiQvACAACICcELAAAgJgQvAACAmBC8AKSemW0zsy+Y2ZSZvSPp8ayXmbmZXZ70OADEj+AFYE3MbL+ZLZjZZNPxu8NAsS98vdvM/sHMTprZGTP7ppm9LnxvX3judNPjlW0u+wZJJyWNu/sv9eHP8Doz+9J6Pyf8rP1m9vx+fFaLz67/PRWi+HwA8eM/ZgC9+K6kV0v6M0kys2sklZvO+ZCkb0jaK2le0jWStjeds9Hdl7q43l5J93sPHZ/NrNDlNQAgclS8APTiQ5L+z4bXN0n666ZzniHpA+5+zt2X3P3r7v7JtV7IzD4Qfv6bw6rY881syMzeZWZHwse7zGwoPP85ZvaImf2qmT0q6a+aPu9Jkv6bpGeGn3c6PD5kZn9iZgfN7JiZ/TczK4fvTZrZ/zaz02b2uJl90cxyZvYhSXsk/a/ws97c5s/wK2Z2NBzrTze991Iz+7qZnTWzQ2b2toa3vxD+PB1+/jPN7DIz+6yZPRZWEz9sZhvX+vcKIBkELwC9+IqkcTN7kpnlJb1S0t+0OOc9ZvYqM9vT64Xc/XWSPizpj9294u7/Iuk3JN0o6VpJT5V0vaTfbPi17ZI2K6iUvaHp8x6Q9DOSbgs/b2P41h9J+p7wMy+XtEvSW8P3fknSI5K2SNom6deDj/LXSjoo6UfCz/rj5vGb2Ysk/bKkF0i6QlLztOQ5BSF2o6SXSvpZM3tF+N6zw58bw8+/TZJJ+gNJOyU9SdIlkt628m8OQBoRvAD0ql71eoGkb0k63PT+j0n6oqTfkvTdcA3YM5rOORlWkeqPJ3V57ddIeru7H3f3E5J+R9JrG96vSfptd59399nVPszMTNJ/kvT/uPvj7j4l6fclvSo8ZVHSDkl73X3R3b+4hmnPH5f0V+5+r7ufU1NIcvfPu/s33b3m7vdI+oikH2j3Ye7+oLt/JvyznZD0zk7nA0gXgheAXn1I0k9Iep1WTjPK3U+5+1vc/ckKqkR3S/rHMOTUTbr7xobHA11ee6ekAw2vD4TH6k64+1z3fxRtkTQi6a56CJT0qfC4JP0XSQ9K+rSZPWxmb1nDZ++UdKhprOeZ2Q1m9jkzO2FmZxRU4y64caHp/K1m9ndmdtjMziqoNLY9H0C6ELwA9MTdDyhYZP8SSR9d5dyTkv5EQQjZ3IfLH1EwjVi3Jzx2/pKr/H7z+yclzUp6ckMI3ODuFUly9yl3/yV3v1TSj0j6RTP7wS6vdVTBdGDjWBv9raRPSLrE3TcoWH9WD6etPvsPwuNPcfdxST/ZcD6AlCN4AViP10t6XjiFdgEz+yMzu9rMCmY2JulnJT3o7o/14bofkfSbZrYlbGvxVq1cY9bJMUm7zawkSe5ek/SXkv7UzLaG499lZi8Mn/+wmV0eVuvOSqqGj/pnXdrhWn8v6XVmdpWZjUj67ab3xyQ97u5zZna9gipi3QkF06aXNp0/rWDB/S5Jv7KGPzeAhBG8APTM3R9y9zvbvD0i6WOSTkt6WEGF6mVN59Tv1qs/frHLS/+upDsl3SPpm5K+Fh7r1mcl3SfpUTM7GR77VQXTiV8Jp/D+RdITw/euCF9PS7pN0l+4++fD9/5AQQg8bWa/3Hyh8E7Od4XXfDD82ej/lvR2M5tSECD/vuF3ZyT9nqQvh59/o4L1bE+TdEbSP2mVaiOAdLEe2uIAAACgB1S8AAAAYkLwAgAAiAnBCwAAICYELwAAgJgQvAAAAGJSSHoA3ZicnPR9+/YlPQwAAIBV3XXXXSfdfUur9wYieO3bt0933tmuVRAAAEB6mNmBdu8x1QgAABATghcAAEBMCF4AAAAxIXgBAADEhOAFAAAQE4IXAABATAheAAAAMSF4AQAAxITgBQAAEBOCFwAAQEwIXgAAADEheAEAAMSE4AUAABATghcAAEBMCF4AAAAxIXgBAADEhOAFAAAQE4JXqFqV3JO59sJCcH0AAJBtBK/Q/v3SmTPJXPvQIen06WSuDQAA4kPwCi0tBY8kzM5S8QIA4GJA8ApVq8mFn9lZqVZL5toAACA+BK+Qe3IVr4WF5K4NAADiQ/AKuUvz88lce36e4AUAwMWA4NVgYSH+a7oH111cjP/aAAAgXgSvBkmEn/raMipeAABkH8GrAcELAABEieDVIIk1XktLwR2NTDUCAJB9BK8GSazxqnfMp+IFAED2EbwaJFF1WlqS8nmCFwAAFwOCV4Olpfj3a6xWJTOCFwAAFwOCV4MkutcvLUm5HMELAICLAcGrQRLBa35eKhQIXgAAXAwIXg1qtfgD0MJCELzc45/mBAAA8SJ4NUiq4pXPB6GLjbIBAMg2gleDWi3+4LW4GAQvKf5rAwCAeBG8msQdfubmgsX1ZlS8AADIOoJXk7jXeC0uLq/xIngBAJBtBK8GSfTTmp8PKl4SU40AAGQdwatBPh/vtkG1WvBgqhEAgIsDwatBLhdv8GqsrjHVCABA9hG8GsRd8WqeWmSqEQCAbCN4NUiy4sVUIwAA2UfwapBkxYupRgAAso/g1SCfD9o7xGVpKah0ScncUQkAAOJF8GoQ91Rjtbq8PyPBCwCA7CN4NcjlgjAU15Tf4uJyxSufJ3gBAJB1BK8W4rq7cG5ueZ9Gs3inOQEAQPwIXi3EFbwaN8jO5ah4AQCQdQSvFuKseNW3CyJ4AQCQfQSvFuKseBUKwfNcjqlGAACyjuDVQlzBq3GDbO5qBAAg+wheLcQVgBYWltd45fNsGQQAQNYRvJqYxROA3IOA13hXIxUvAACyjeDVQhwBqLF5qsQaLwAALgYEryb5fLD2KmrN4S6XC4JYYxgDAADZElnwMrNLzOxzZvaAmd1nZm8Mj282s8+Y2XfCn5uiGkMv4to2qNV0JhtlAwCQbVFWvJYk/ZK7P0nSjZJ+zsyukvQWSbe4+xWSbglfp0Y+H0/watwguxEL7AEAyK7Igpe7H3X3r4XPpyQ9IGmXpJdL+mB42gclvSKqMfQin49nrVXzGq86Kl4AAGRXLGu8zGyfpO+VdLukbe5+VArCmaStcYyhW3FNNbareBG8AADIrsiDl5lVJP2DpDe5+9k1/N4bzOxOM7vzxIkT0Q2wSZyL61tVvJhqBAAguyINXmZWVBC6PuzuHw0PHzOzHeH7OyQdb/W77v5ed7/O3a/bsmVLlMO8QFxtHRqbpzai4gUAQHZFeVejSXqfpAfc/Z0Nb31C0k3h85skfTyqMfQilwvCT9QBqHGD7EYELwAAsqsQ4Wc/S9JrJX3TzO4Oj/26pD+U9Pdm9npJByX9WIRj6Fm12joY9UvjBtmNCF4AAGRXZMHL3b8kqcXycUnSD0Z13X6pVqViMbrPb9wgu/m6AAAgm+hc30bUAYg1XgAAXHwIXm1EvV/j/PzK4BXXBt0AACAZBK8W4ghArSpeZmyUDQBAlhG8WnCPNnjVu9Y3N1DN56OvtAEAgOQQvNqIOni1YkbwAgAgywheLZhFu21Qu3AVV/NWAACQDIJXC/l8tMGrXcUrl6PiBQBAlhG8Wsjno608tdsgm+AFAEC2EbxayOWi3Si73QbZ3NUIAEC2EbxaiGOqsVXFK5+njxcAAFlG8Goh6kXu7UIddzUCAJBtBK8Woq54LSy03iCbuxoBAMg2glcLUS+ub7dBdi4X7NXYav0XAAAYfASvFsyCABTVhtXtNsiuY6NsAACyieDVQVQL3VttkN2I4AUAQDYRvDqIaqH7ahUv7mwEACCbCF5tmEUXgJhqBADg4kTwasM9muDlHlTSWi2uryN4AQCQTQSvDqIIXt1MXzLVCABANhG8OohijVc3oYqKFwAA2UTw6iCqiler7YIaEbwAAMgmglcbUXWRr1ZXb5DKVCMAANlE8Gojnw/6bfXbahWvevNWAACQPQSvNqKqeC0tUfECAOBiRfBqI5dLruLFRtkAAGQTwauNQiGaANRug+y6fD66jvkAACBZBK82oppqXK1rvRnBCwCArCJ4tZHLBSGp31bbIDuqwAcAAJJH8Gojn48meK1W8crlqHgBAJBVBK82zKLZr5HgBQDAxYvg1YFZNMGr0+J67moEACC7CF4dJFHxyufp4wUAQFYRvFbRzxBU3y5otT5eTDUCAJBNBK9V9DMEdbNBNnc1AgCQXQSvVURR8eoklwv2alztPAAAMHgIXqvoZ/DqpuIlsVE2AABZRfDqoN93NXazQbYUnEPwAgAgewheHZj1t4lqtdpdxat+LgAAyBaCVwf5fLDFT790W/FiqhEAgGwieHWQz/f3DsPFxc7NU+uYagQAIJsIXh30e6PsubnOzVMbMdUIAED2ELw66PdG2YuL3QUvphoBAMgmglcH/a54zc8z1QgAwMWM4NVBv7vIz88z1QgAwMWM4NVBvxfXLyxIhcLq5zHVCABANhG8JH35wZN69233aKkp7ZgF0379qj51O9UoEbwAAMgigpekh05M69MPHdJMm/JWP4JXrRY8ugle/W7cCgAA0oHgJWlDuShJmm4TvJaW1n+NtYS3fL4/1wQAAOlC8JI0HgavKCte3W6QLQXnEbwAAMgegpeWK17nllYGr35tlN3tdkFS/++mBAAA6UDwUkPwapF2+rW4fi0bZFPxAgAgmwhekjbGNNXYbcUrn6ePFwAAWUTw0vIar1YVLyn+xfVmTDUCAJBFBC9JxXxO5UK+ZcWrX/s1Li52P9WYyzHVCABAFhG8QpVSsWU7iX7t17iw0P12QQQvAACyieAVGi0VNdPirsZ+VbzW0rWe4AUAQDYRvEKVUrHlGq9+VbzWskF2LhesCet2MT4AABgMBK9QZagY6RqvbjfIrmOjbAAAsofgFWpX8crn+3OH4cJC91ONUlDtIngBAJAtBK/QWKnYsnN9Eovr6+jlBQBAthC8QpVSUQvVqhabykz1DavXs97KPaiarSV4MdUIAED2ELxClVLYRHWh9bZB6wlBvVSumGoEACB7CF6hevCabjOvuJ72Dr3eochUIwAA2ULwClWG6sFrZcXLbH0haGmp+671jah4AQCQLQSv0Pmpxha3MLqvP3j1UvEieAEAkC0Er9DyVGPr3hHrCV7VKhUvAABA8DqvUipJar24XlrfGq9eK16s8QIAIFsIXqFKKWgrP724MmGtd41XLxUv2kkAAJA9BK9QPpfTcL4QScVrfr634NWPjvkAACA9CF4NRovFlmu81tu9vpeu9bkcwQsAgKwheDUYKRZb9vHK5YKqVa/m59e2QXb9muupsgEAgPQheDUYLRYi2Sh7rRtkSwQvAACyiODVoN1U43qD1/z82qcaWeMFAED2ELwajBSKLRfX92Oqca3BK5+nnQQAAFkTWfAys/eb2XEzu7fh2NvM7LCZ3R0+XhLV9XsxWixpOoKpxsVFKl4AACDaitcHJL2oxfE/dfdrw8fNEV5/zUaLRS1Ua1psKjWt567GWi14rLWdBGu8AADInsiCl7t/QdLjUX1+FEaL4bZBTaWm+rRfL93ne90gm+AFAED2JLHG6+fN7J5wKnJTu5PM7A1mdqeZ3XnixIlYBjZS7P9+jb1uF0TwAgAge+IOXv9V0mWSrpV0VNI72p3o7u919+vc/botW7bEMrjRQhC8Wi2wd+8tePW6QXYu13uVDQAApFOswcvdj7l71d1rkv5S0vVxXn81oymqeNWxXyMAANkRa/Aysx0NL/+9pHvbnZuEkTZrvOp6mfpbb0sIghcAANmxxo1sumdmH5H0HEmTZvaIpN+W9Bwzu1aSS9ov6f+K6vq9qBTbTzVKvVe81oPgBQBAdkQWvNz91S0Ovy+q6/XDSETBq5c1Xuu5JgAASCc61zfImWmkUGg71dhLCJqbW3vz1EZUvAAAyA6CV5PRUuttg8x6mzZcWCB4AQCAAMGrSaVU1FQf92vsZZ/GRkw1AgCQHQSvJqPF1hWvfL63bYOoeAEAgDqCV5PRUrHtRtkELwAAsB4EryaVNmu8cjmpzZr7jubng9/tFVONAABkB8GrSafgtdY1XvVthnqteJlR8QIAIEsIXk0qxaIWajUtNJWaCoW1V7zWu9eiWW9VNgAAkE4dg5eZPdPM3mNm95jZCTM7aGY3m9nPmdmGuAYZp9FS6yaqvUw1rrd5aq/TmwAAIJ3aBi8z+6Sk/yjpnyW9SNIOSVdJ+k1Jw5I+bmYvi2OQcaqUWu/XmMutfcPr9W6QXb8mAADIhk5bBr3W3U82HZuW9LXw8Q4zm4xsZAkZrW+U3WHboEKXGy2td2E8wQsAgGzpNNV4PlSZ2VDjG2Z2oyS1CGYD73zFq03wWksQWm9oYo0XAADZ0il4/W3D89ua3vuLCMaSCu3WeNWtpYq13opXPk87CQAAsqRT8LI2z1u9zox2a7zq1hKEqHgBAIBGnYKXt3ne6nVmjBT7V/Fa7z6NrPECACBbOi0T321m71ZQ3ao/V/h6V+QjS0jeTCPFQsfF9d0ieAEAgEadgtevNDy/s+m95teZUikWda7NHN9agtB692nM5VjjBQBAlrQNXu7+weZjZrZJ0mn39XSnSr/RUrFlxcss/uBV7wW2nkasAAAgHTo1UH2rmV0ZPh8ys89KekjSMTN7flwDTEKlTfDK59e2X+N6N8iuY79GAACyoVMseKWkb4fPb1KwtmuLpB+Q9PsRjytRo8X2G2Wv5S7D9Va8JDbKBgAgSzoFr4WGKcUXSvo7d6+6+wPqvDZs4FVKxZbtJNZS8ZqbW//ieimYZiR4AQCQDZ2C17yZXW1mWyQ9V9KnG94biXZYyaqUWle88vnuKl7u0v33S8Vif9ZmscAeAIBs6BS83iTpf0r6lqQ/dffvSpKZvUTS16MfWnJGS0Ut1mqab0o8uVwwfbiaI0ekRx+VNm5c/1iYagQAIDs63dX4FUlXtjh+s6SboxxU0ioNTVSHystzhfm8NDvb+XdnZqR775U2b+7PWJhqBAAgO9oGLzP7xU6/6O7v7P9w0mG0YaPszeXh88dXW1xfq0n33RdMMYbZrS+YagQAIBs6LZL/E0l3S/qkpHlleH/GZpWG4NWo3tC0VmvdJuLQIenECWnbtv6NhalGAACyo1PwepqkV0l6qaS7JH1E0i1Zb54qBe0kJLXtXl+trgxe09PSAw9IExP9HQtTjQAAZEfbxfXufre7v8Xdr5X0Pkkvl3S/mb0srsElpV3Fq6556q9Wk775TWl4WCpE0GiDqUYAALJh1b7qYTuJ75V0jaRHJB2PelBJq6/xatVSQloZhPbvl06flsbG+j8WphoBAMiOTovrf0pB9/phBW0lftzdMx+6JGmkWJRJLZuoShcGr7NnpW9/u/9TjHVr3R8SAACkV6eJsfdJ+qakgwo61/+QNXQDdffMTjnmzTRSLLSteNWDULUq3XOPNDq6/g717XTbOwwAAKRfp+D13NhGkUKjbTbKNluueD30ULCofsuW6MaRy1HxAgAgKzo1UL01zoGkTaVYbHlXo3sQvE6dCoLX5GS046i3sAAAAIOv7eJ6M/tfZvYjZraiFaiZXWpmbzezn452eMkJKl4r5/jMgu7099wTLKZv1c+rn8yYagQAICs6TTX+J0m/KOldZva4pBMKFto/QdKDkv7c3T8e/RCTUSkVdfDM3Irj+bx0+LA0Nxd9tat+PSpeAABkQ6epxkclvVnSm81sn6QdkmYl/Zu7z8QzvOSMFos6t7BycVWxGEwz7twZzzi4qxEAgOzopo/XqKSD7n6bpBlJz281/Zg1lVJJ04uLam7UX6kEocti2kCJxfUAAGRHNyuUviBp2Mx2SbpF0k9J+kCUg0qDSqmopVpNC9WV3UvjCl3S6htzAwCAwdFN8LJwavH/kPRn7v7vJV0V7bCSV+9e366Jaly4qxEAgOzoKniZ2TMlvUbSP4XHItiRMF0qxc7bBsWlPtWY/a3JAQDIvm6C15sk/Zqkj7n7fWZ2qaTPRTqqFKhXvKZS0suB/RoBABh8q1auwkaqt0qSmeUknXT3X4h6YEmrrLJRdpzqG2VHtS0RAACIRzd3Nf6tmY2HdzfeL+nbZvYr0Q8tWaPFdKzxkoJpRipeAAAMvm6mGq9y97OSXiHpZkl7JL02ykGlQZoqXhIL7AEAyIJuglcx7Nv1Ckkfd/dFSZlf6j1SLMiklhtlx60+1QgAAAZbN8Hr/5O0X9KopC+Y2V5JZ6McVBrkzILu9Uw1AgCAPulmcf27Jb274dABM3tudENKj9FSkalGAADQN90srt9gZu80szvDxzsUVL8yr1IqMtUIAAD6ppupxvdLmpL04+HjrKS/inJQaTFaTEfwYqoRAIBs6KYD/WXu/qMNr3/HzO6OaDypMloq6uTsbNLDkETwAgAgC7qpeM2a2ffXX5jZsySlI41ErJKSNV5mrPECACALuql4/YykvzazDeHrU5Juim5I6VEPXu4uM0tsHGbBfo0AAGCwdXNX4zckPdXMxsPXZ83sTZLuiXhsiasUi1py13y1quFCcvuC53JSCrpaAACAdepmqlFSELjCDvaS9IsRjSdVRlPSvd6M4AUAQBZ0HbyaJDfvFqP6tkFJ79eYz7PGCwCALOg1eGV+yyCpYaNsKl4AAKAP2i5cMrMptQ5YJqkc2YgSks+vbNlwvuKVcPDK5VhcDwBAFrQNXu4+FudAkjY0tDJ4pWWNF8ELAIBs6HWqMXOGhlauo6oU07HGi+AFAEA2ELxChUKwNU+jcrGgnNJR8WKNFwAAg4/gFSoUgkXsjXJmGknBRtm5HHc1AgCQBQSvUKuKlxRMN55jqhEAAPRBL3c1SpLcfTySESUkn299fDQFFa+6Wi0IYQAAYDCtelejmb1d0qOSPqSglcRrJGXujsd2OwJVUhS8qlWCFwAAg6ybf8Zf6O5/4e5T4bZB/1XSj0Y9sLi1C16jxWLii+vrmttdAACAwdJN8Kqa2WvMLG9mOTN7jaTMLfXO51curpfCildKbikkeAEAMNi6CV4/IenHJR0LHz8WHssUs6DqtaKXVymoeHmrlfcx485GAAAGW9s1XnXuvl/Sy6MfSvJKpSDcNC60Hy0VVXXXXLWqcrv5yHVYrFb1rcdO6Zqtk6ueS8ULAIDBtmrFy8y+x8xuMbN7w9dPMbPfjH5o8Wu1bVC9e31U67w+d+Cw3nbr7Tp+bmbVcwleAAAMtm6mGv9S0q9JWpQkd79H0quiHFRSSqWV/bJGI94oe//pM5KkY10EL6YaAQAYbN0ErxF3v6PpWCbbebbcr7G+UXZEC+wPnpmSJD02M9fxPDMqXgAADLpugtdJM7tMYTNVM/sPko5GOqqEtJpqHC1GV/Fy9+XgNds5eLkTvAAAGHTdrBb/OUnvlXSlmR2W9F0FTVQzp1PFK4rg9fjcvM4tBsXDkzOzq57PVCMAAIOtY8XLzPKSftbdny9pi6Qr3f373f3Aah9sZu83s+P1Rfnhsc1m9hkz+074c9O6/wR91Gq/xvoarygW19erXTmtXvFiv0YAAAZfx+Dl7lVJTw+fn3P3qTV89gckvajp2Fsk3eLuV0i6JXydGq26RYwUCsqZRdJE9VAYvK6Y2LhqxSuXk1LSxxUAAPSom6nGr5vZJyT9D0nn6gfd/aOdfsndv2Bm+5oOv1zSc8LnH5T0eUm/2uVYI9cqeJmZRouFaCpeZ6e0cXhI+zaO68uHOi+bMyN4AQAw6LoJXpslPSbpeQ3HXFLH4NXGNnc/KknuftTMtvbwGZFpbJzaaDSijbIPnpnSnvExTZbLml5Y1PxSVUOF1oPI51njBQDAoOumc/1PxTGQZmb2BklvkKQ9e/bEcs12jekrxWLf20nU3HXo7JR+6NK9mhgZliSdnJ3VrrFKy/OpeAEAMPhWDV5mNizp9ZKeLGm4ftzdf7qH6x0zsx1htWuHpOPtTnT39yq4m1LXXXddLBsltgteUVS8jp+b0UK1pj0bxjRRLksKenm1C14srgcAYPB108frQ5K2S3qhpFsl7Za0lkX2jT4h6abw+U2SPt7j50Qinw8qS80qEQSvA+HC+j0bxjTZUPFqh+AFAMDg6yZ4Xe7uvyXpnLt/UNJLJV2z2i+Z2Uck3SbpiWb2iJm9XtIfSnqBmX1H0gvC16lhFlS9mtdSjRaLfV9cX7+jcfd4RZvLQfDq1L2e4AUAwODrZnF9PXGcNrOrJT0qad9qv+Tur27z1g92N7RklEpB8GpcaF8pFTW9uCh3l7UqifXg4NkpbR0tqxzOb44PlTq2lMjlpLnOrb4AAEDKdVPxem/Y6PS3FEwV3i/pjyMdVYJabRtUKRVVc9fcUv9uK6zf0Vg3WR7u2EQ1l+OuRgAABl03dzX+9/DprZIujXY4ySuVpOalVvXu9dOLiyoXuykSdrZYq+nI1Dk9Y+e288cmRso6Nj3T9ncIXgAADL5u7mp8a6vj7v72/g8neS33aywubxu0ZaS87mscnTqnqrv2bGioeI0M674Tj3X8vfpG2blu6pQAACB1uvkn/FzDoyrpxepijdegajXVONrnjbLrezRe0jDVOFEua2ZxSbOLnVfQU/UCAGBwdTPV+I7G12b2JwrWemVSy4rX+eC10JdrHDw7pbyZdo0v9+yqt5R4bHZWu4tjLX/PbGUoBAAAg6OXSasRZXitV6EQTOk1Gi0ur/Hqh4NnprRjbFTFhjnDehPVkx1aStSnGgEAwGDqZo3XNxXszShJeUlbJGVyfZfUunt9veLVr15eB89M6bJNGy441k0TVYmpRgAABlk3t+j9cMPzJUnH3D2zrTxbBa9yoaCcWV/WeM0tLenYuRk9d9/uC45vKg/LJD3eoeLFVCMAAIOtm+DVvD3QeGMTUXd/vK8jSlhj49Q6M+vbRtmPnJ2WpAvuaJSkYi6nDcNDOtmhlxdTjQAADLZugtfXJF0i6ZQkk7RR0sHwPVfG1nt13ih7/YW+Vnc01k2Whzt2r5eYagQAYJB1s7j+U5J+xN0n3X1CwdTjR939Ce6eqdAltQ9elVJ/9ms8eHZKpVxO2yojK96bGBnuuF8jU40AAAy2boLXM9z95voLd/+kpB+IbkjJyueDgNNstE9TjQfPTGn3eEX5FheZKJc7bhvEVCMAAIOtm+B10sx+08z2mdleM/sNSZ1brA8ws6Dq1TylN1oqaqoPfbwOnZlasb6rbnJkWLNLSx0DHlONAAAMrm6C16sVtJD4mKR/lLQ1PJZZpVKrJqqFdU81Ti0s6PG5eV3SNngFvbzaTTfmctJSZu8nBQAg+7rpXP+4pDdKkpltknTavbnFaLYMDUnNRadKqaRzi0tyd1mrucguHDoT3NG4t03wmigvd69vVRUjeAEAMNjaVrzM7K1mdmX4fMjMPivpQUnHzOz5cQ0wCaXSyoBTKRZVc9fsOpLPwTNnJUl7WtzRKEkTI52715tJfdq1CAAAJKDTVOMrJX07fH5TeO5WBQvrfz/icSWq1X6N/dgo++DZKY0UC9ocVraabRoeUk5q21Iin2eNFwAAg6xT8FpomFJ8oaSPuHvV3R9Qd/2/BtbQ0Mq7B89vG7SOOxsPnpnWnvGxtlOVhVxOG4eH2t7ZSMULAIDB1il4zZvZ1Wa2RdJzJX264b2VTagypGXFq7i+ipe7d7yjsW5ipNx2cX2xKM3M9HR5AACQAp2C1xsl/U9J35L0p+7+XUkys5dI+noMY0tMoRD0zGpUWedU46m5eU0vLra9o7FucmS47UbZxaJ07lxPlwcAACnQdsrQ3W+XdGWL4zdLunnlb2RHq+719TVevbaUqG8V1G5hfd1EuayvHT3R8u7JXC4IhIuLQQgDAACDpZs+XhedVsGrUp9q7HGN18Gz4R6NGyodz5sYGdZ8tdr2Ou6s8wIAYFARvFrI51ceGy7klTfrueJ16MyUNg4NacPQUMfzJsudm6hK0vx8T0MAAAAJI3i10KriZWYaLRV7XuN18MzUqtUuKVjjJQVNVNuh4gUAwGDqqi2EmX2fpH2N57v7X0c0psS1qnhJwXRjL+0kau46dHZaL7h0z6rn1rvXt2uims9zZyMAAINq1eBlZh+SdJmkuyXVmyy4pMwGr0Ih6JnVrNeK1/FzM5qvVruqeG0sDytn1raXV6nEnY0AAAyqbipe10m6Kuv7MzYyC8JXtXph9atSKmpqfu3zfN3e0ShJeTNtHh7SY2261xcK0tTUmocAAABSoJs1XvdK2h71QNKmVGrdRLWXitf5Oxq7CF5S0ES13VQjTVQBABhc3VS8JiXdb2Z3SDp/P527vyyyUaXA0FDQL6tRpVTsqZ3EwTPT2jpSVrnY3U5LE+VhPXz6TMv38vlgA++lpdY3AQAAgPTq5p/ut0U9iDQqlaTmGwsrpaJmFhZVc1euzX6LrRw6M7Vqx/pGkyPDuvPIsZZNVOsWFgheAAAMmlX/6Xb3W+MYSNq03K+xVFRN0uzS0vm9G1ezWKvp8NS0nr5za9fXnhgpa6FW09TCosaHSi3PWViQRjK9YyYAANmz6hovM7vRzL5qZtNmtmBmVTM7G8fgkjQ0JNVqFx6rd69fSxPVo1PnVHXvamF93XJLifa9vGiiCgDA4Olmcf2fS3q1pO9IKkv6j+GxTGtX8ZLWtlH2oXBh/Z41TTWG3evbtJTI5VZOgwIAgPTrapWQuz9oZnl3r0r6KzP714jHlbhCIdgXsVGlvlH2GhbYHzgzpZyZdo2Ndv07k2HFq11LiWKRXl4AAAyiboLXjJmVJN1tZn8s6aik7lPEgGq1cL2+rmstFa+DZ6a0szKqYrt2+C1sGB5S3kwnaaIKAECmdDPV+NrwvJ+XdE7SJZJ+NMpBpUGrnDQWVrzqDVG7sdY7GiUpZ6bN5eGOTVSnp9f0kQAAIAW6uavxgJmVJe1w99+JYUyp0GrboM3lYT1t+xb9j/u/o22VET1n7+6OnzG3tKRj52b0A/s6n9fK5Mhw2zVehUJwV2NzZ30AAJBu3dzV+CMK9mn8VPj6WjP7RMTjSlyrNV5mpl/+vqfr6q0Tes8d39CXDx3p+BmPnJ2WS9ozvvoejc0myu2719ctrH33IgAAkKBuphrfJul6Saclyd3vlrQvqgGlRbtK0lA+r7c86zpdOblZ77r9bn3lkaNtP+NgD3c01k2EFa9ahy0yCV4AAAyWboLXkru33r8mw1pNNdYNFwr69e9/hq7YvFHv/MrX9dUjx1qed+jMlIq5nLZV1n4vwmR5WEu1ms522JSb4AUAwGDpapNsM/sJSXkzu8LM/kxS5ttJmAXhq7mXV125WNBv/Ltn6Akbx/Unt31NXzt6fMU5B89Ma/d4Rfk1bC9UN7FKLy8zenkBADBougle/1nSkxVskP0RSWclvSnCMaVGqdQ+eElBe4nfevYNumS8oj/+17v0jWMnL3j/4JmzPU0zSstNVNt1r6eXFwAAg2fV4OXuM+7+G+7+DHe/LnzeedV3RrTaNqhZpVTUbz/7Bu0YG9Uffvmruu/EY5KkqYUFPT43v6atghpN0EQVAIDMadtOYrU7F939Zf0fTrqUSt1N540NlfS2Z9+gt976Ff3+F7+q33r29aqFa+J7rXiND5VUyOXaTjXSRBUAgMHTqY/XMyUdUjC9eLuktS9UGnCt9mtsZ8PwUBC+Pv8V/e4Xv6pn7t4uqffglTPTRHm4bUuJQiEIhbVasHcjAABIv07/ZG+X9OuSrpb0/0p6gaST7n6ru98ax+CS1s1UY6NN5WG97Tk3asNQSZ/d/4hGCoXzU4a9mCgP67FVSm7c2QgAwOBoG7zcverun3L3myTdKOlBSZ83s/8c2+gSttri+lYmwvC1daSsyzZvkPVwR2Pd5Ej7ilcdwQsAgMHRccsgMxuS9FJJr1bQNPXdkj4a/bDSIdwTe822jJT1jh/6d2rf+rQ7EyNlPT57VDV35doEOIIXAACDo9Pi+g8qmGb8pKTfcfd7YxtVSrTaNqhbI72mtgaT5WFV3XVmbl6bWkxZmknz8+u+DAAAiEmnitdrJZ2T9D2SfqFhyswkubuPRzy2xCW9AXW9ierJ2bmWwatQkKam4h4VAADoVdvg5e4X/b1ynbYNikNjL68rNm9c8X6xKM3MxDwoAADQs4s+XHWynqnGfphsqHi1QhNVAAAGC8Grg6SnGsdKRZVyuY7d62dmkg2HAACgewSvDpKeajQzbe7QUsIsCF2LizEPDAAA9ITg1YFZEL7W2surnybL5bbbBtVxZyMAAIOB4LWKXpqo9tPkyHDbqcY6enkBADAYCF6rWOu2Qf22uTysx+fmVe2wkIvgBQDAYCB4raJYlJaWkrv+5EhZNXednms9n8idjQAADA6C1yqGh5OteE2OLPfyaoUmqgAADA6C1yqGhpJd4zVRDnt5tbmzkSaqAAAMDoLXKpJeXD8RVrxOzrbv5cVUIwAAg4HgtYo+7HW9LpViUUP5vB5rU/HK5YKpUHp5AQCQfgSvVSS9bZCZaWJkWI+1qXjVcWcjAADpR/BaRdLbBknBZtnt1njV0UQVAID0I3itIultg6SgpQRNVAEAGHwEr1UkPdUoBRWvU3PzWmrT1yKf585GAAAGAcFrFamYahwpyyWdatNEtVTizkYAAAYBwWsVqZhqLNNEFQCALCB4rcIsqHolu1F20ET1sVmaqAIAMMgIXl1IeqPs801U21S88vlgP8kk95QEAACrI3h1Ieltg0YKBQ0X8qu2lODORgAA0o3g1YViMdlqkplpslymiSoAAAOO4NWF4eFkpxqlYLqx3bZBdTRRBQAg3QheXUh6o2wpWGB/ss3ieinYs3GVghgAAEhYIYmLmtl+SVOSqpKW3P26JMbRraTXeElBE9Uzc/NarNVUzK3My8UivbwAAEi7RIJX6LnufjLB63etWEx6BMFUo0s6NTunraMjK96niSoAAOnHVGMX0rBt0GQ56OXV7s7GQkGano5zRAAAYK2SCl4u6dNmdpeZvaHVCWb2BjO708zuPHHiRMzDu1A6tg0Ku9e3WchVKAR3NSY9JQoAANpLKng9y92fJunFkn7OzJ7dfIK7v9fdr3P367Zs2RL/CBukY9ugoOJ1dLpzi3paSgAAkF6JBC93PxL+PC7pY5KuT2Ic3UrDVGO5WNBlmzboa0ePdzyP4AUAQHrFHrzMbNTMxurPJf2QpHvjHsdapGGqUZJu2LVd33n8dNs9G80IXgAApFkSFa9tkr5kZt+QdIekf3L3TyUwjq6lYapRkm7YtU2SdMfhR9ueQy8vAADSK/Z2Eu7+sKSnxn3d9TALql7VarLVr93jY9o5NqrbDz+qF1++b8X79PICACDdaCfRpaGh5LcNkoLpxvtOPK6pFnOKBC8AANKN4NWlNGwbJAXBq+auu46sXGRPE1UAANKN4NWlUklaWkp6FNJlmzZoc3lYt7dY51UoSHNz6ajMAQCAlZLcMmigDA9Lp08nPQopZ6brd27TZ/cf0vxSVUOFCxeduQd3Ng4PJzRAoM86tXLpps3Lauf0+vn9uHbWXex/fqRTqSS12PI4NgSvLqVlqlGSrt+1XZ966IDuPnZCN+zavuJ9gtfFwT2obrb6uZZj1Wrwulpdft7t68bPax5b489WY1/tz9aKWef3Vvvs9fx+P67dbwQbYG1qNemqq6R9+5IbA8GrS0ND6QleT96yWZViUXccfrRt8EL06oGj8dF4rNPzajWYuq7Vln/Wg02rIFR/7h6cXw9NrbQKCJ2OmbV+5HLLYaH+vP7I55fbrNQfq1ntnDS0bAGQbadOJb8ch+DVpWIx6REsK+RyevrOrbrzyHEt1WoqNNRMzaT5+QQHlyELC9LJk9Lhw9Li4nJYagxBUudKSDvN4abxtRT8bHyez7c+DwAwWAheXUrDtkGNbti1XbceOKz7Tzyup2ybPH+8UJCmpxMc2IBzl86cCcLWI48Er0dHg+BTLC6vDei2ygMAQCOCV5fSsm1Q3bXbtqiUz+n2w49eELzo5dWbhQXp+HHp4YeDv79SSdq8mcoSAKC/CF5dSsu2QXVDhbyu3bZFdxw5ptd/75OVCwdH8Opevbp16JB05EjwemxM2ro16ZEBALKK4NWltE01SsF04x1HjumhU2d0xeaNkoLgdfbs8qJprDQ/v1zdmpkJbpygugUAiAPBq0tpm2qUpKfv3KqcmW4//Oj54FUPW4uLwXQZAu5BH7ZHHgnWb5lR3QIAxI/g1aVCCv+mxkolPXnLZt1++FH95DVXnj/uHlR1CF7B2q1jx6TvfjeYgh0akiYmqG4BAJKRwjiRTmZB+KpW01X9un7Xdr3v6/fpkbNT2j0+dv74xdzLq9XarfFxqlsAgOTx//1rMDSUfOO1Zjfs3CZJuv3wsQuOX4zBa34+mEr84hel224L1nFt3ixt2RJ8dwAAJI2K1xrUN8pOUzPViZGyLt+0QXccflQ/+qTLJQWVuYvlzsZaLehEfOiQ9Gi4bzhrtwAAaUXwWoNSSZqbS3oUK92wa7s+fO+39djMrCZGyioWs99EdXo6CFoHDizvTTk5yZ2cAIB0Y6pxDYaH0zfVKAXrvCTp9iPBdGNWe3ktLkpHjwbTiF/8YtAOolIJqlvj44QuAED6UfFag1IpPRtlN9o9XtGusVHdcfhRveTyfSoWg+m3LKhWgzYQR44Ej1qNqUQAwOAieK3B0FA6g5cUTDf+47cf1tT8gsaGSlpclB58UHrCE9J1F2Y3lpaWw9ajjwZ/5zQ5BQBkAcFrDdK0qL7ZDbu266Pfekh3Hj2u5+7bra1bpe98RzpxQnrKU4KNntNscTGo0h05EvTdqtWCqd1NmwhbAIDsIHitQRq3Daq7bNMGTZSHdfvhR/XcfbuVywXTcVNTwXqoa66Rdu5M1zqoubmg39bhw0HrB3epXKayBQDILoLXGqR5ys7MdP2ubbrl4UOaW1rScNhqf2wsCDPf+EYQbq66KpmeVu5B0Jqakh57LBjL7GzwXrnMHYkAgIsDwWsN0rhtUKPrd27XJx88oLsfPaEbd+84f7xQkLZtCwLPl74kXXttsG1OlNyDDainpoLpzhMnlpu6FovB1GelEu0Y0qjmrqq7am0fuuC1tznefL6r/ed44+/KVxy/4Bpa+TveMGZX4+8sn9NO8Bvh8w7V4ua3vMvScuNZnX6n1TvtT/cWz1r9zsoPWNt10mVAhgmsy+KC9NrRfbr00k2JjSHlUSJd0h68rtqyWZViUbcfPnZB8KrbtCmoOt1+u3TppdLll6//z1SrBR3j5+eDz56els6eDUJe/UaEoaEgaG3YsL5rRc3ddXp+XkemzoWPaR2ZOqdzi0vnA0a1VlPNparXLgxRtVaBqn7e6gEl7UxSzkxmppxJOVn4Oqi22gXn2spfXvl05bntf63jscZS6crPb31ex89b8XvN77W/XruDK/5OUmowRgn0rlqTpucXEx1DyqNEuqR5qlGSCrmcrtu5VV89ckxLtZoKLRZKDQ8HbTH275dOnpSe+tRgOrJRrRY8qtXl57VacLfh/PxyuJqaCsJWY54oFIKK1oYN6f37ml+q6uj0crg63BCyZpaWzp9XyuW0fWxU46WSSvmccmbnH/n6z9yFr9s/FJ6fO/98tXPrn2taedzanH/+fTUfU4vfuzBE1YPUimsoCFcAMOhOnZIu3ZPsGAhea5D2ipcUNFP9/IHDuu/E43rqtsmW5+Rywf6F09PBwvsNG4K7CpeWlqtUjWHK7MLXxWLwGBpK73Shu+vx2bnzoerw1DkdDsPVyZnZC6ZVJkfK2lkZ1bP37tKusVHtHKto59ioJkfKyhE4AAB9NABRIj3MgvBVraa3mnPtti0q5XP60sHDesrWiY6VikolWNherQY/zYJQNkhZY3phUUenz+no1LnzVazDU9M6OnVOcw1N14YLee0aq+jKyU3aNXaJdo6NatdYRTsqoxoqpPTLBABkDsFrjYaGgmm3tAavoUJez9y9Q5/d/4j+7fHTevHl+/QDe3ep3KZcl8+n989Sd25xUY9Oz5wPV41Ba2phea7eJG0ZKWvnWEVXbdmsnWNBR/9dYxVtGh5iugwAkDiC1xqVSsGUXJqbqf7M06/R1VsmdPOD+/WXX7tXH77nW3rOvt160eV7tWssfXODs4tLOj4zo+PnZnX8XPDzRMPrc4tLF5w/UR7Wjsqobty9QzsqI9pRGdWOsVFtGx1RKe0pEgBwUSN4rVGpFCwoT7NSPq/nPeESPXffbv3b46f1yQf369MPHdDND+7XU7dN6sWX79PTdmxVPuIK0EK1qtNz8zo1Nx/8nJ1bfj43r8dn53RiZlbTCxfeYTKUz2vLaFlbR8p64sQmbRkta/voiHaMjWr7KFODAIDBRfBao+HhYB/BQWBmeuLEJj1xYpNe99Sr9C8PH9Q/P3RAf/jlO7V1pKwXXr5Xz9t3icrFgmo115LXzrdFqNZqqrprqeaqeU1LNdfc0pJmFpd0bnFRM4tL4WNxxeup+UWdmptbUamSpJyk8eEhbQof3zOxUVtHRrR1dERbR8vaMlrWeKnEtCAAIJMIXmtUKqV3o+xONg4P6T9cdYVeceVl+uqRY/rkg/v1oXu+pQ/d8611fW7eTCPFgkaKRY0UCxotFrVrvKKrt05oU3lIm4aHtbEetMpDGh8airzSBgBAWhG81mhoaDCDV10hl9Mzd+/QM3fv0IEzZ3XnkeNyuQqWUy5nKpgpl8uFP00FyymfC/pJDRcK50PWaPizlM9RnQIAoEsErzUqFgdnC5DV7N0wrr0bxpMeBgAAF42Vrc3R0SA0UQUAAOlE8FqjoaHsVLwAAEC8CF5rNDoahK+llTfspQrhEACA9GHibI3MpO3bpaNHpY0bkx5Na0tL0okTK6dF6xtYl0rBT9bEAwAQL4JXD7ZulQ4cSHoU7Z06JV19tXTJJdL8/PJjelqamgp+nj3bvipW3xS7+Wer85of9eP5fOufg7YXJAAA/UTw6sH4+IWhJE0WF4Nq1q5dQdgZGQkezdylhYUgkFWrFwar+vNWPxsf1Wqwb2X90fh6aSkYy9JS8JibW36+2jRoff/IVg8AAAYZwasHpZK0aVMQJsrlpEdzoVOnpGuuWX0vSbNgrdrQUDzjatQY0qrV5UBWfywsBH+3jdW6c+eW19U1V+Dy+WAatf6z/hwAgLQhePVo507p/vvTFbzqQXDHjqRH0lkuFzzWqlZbrqI1/pydDcLZ7GzwdzA9HYS3ZmYrA1qhkL6qJQAguwhePdq4MQgCaXL2rPS0p2W32pPLdV+lq4e0xoC2sBCEs8bHmTPB+Y0VtHoQawxnze81Pm/+2azV++2eAwCyjeDVo0olmHJcWkpHU9WZmWDt2datSY8kHdYS0poraIuLF1bQ5ueDIFdf2yatXO/W/KhrPr/5d5vPa9Q4pdrN81708rnrveZaxtYsyTYpaRsPgLVrNRsStxREhsFkFkzppaWtxNSUdMMNVE960VzVSlK7MLba8+YA0OvnrOWzWr1ezSAElUEYI4DetbrhLE4p+edmMKWlrcT0tDQ5KW3enPRIsF5MQQJAttG5fh0a20okxT2YZnziE/mHGgCAtCN4rUNjW4mknDkTTHlu2JDcGAAAQHcIXuu0Y0fQYyoJ9Saol1+ezPUBAMDaELzWadOm5KYaT5+W9u4N7rAEAADpR/Bap0ol6BJf76oel2o1aHvwhCfEe10AANA7gtc61dtKxD3dePp0MMWYps75AACgM4JXH2zZEjTZjMvSUhD49u6N75oAAGD9CF59sGFDvK0cTp2SrrgiuKsSAAAMDoJXH5RKQfianY3+WgsLwZqy3bujvxYAAOgvglef7NoVzzqvU6ekK69MzxY3AACgewSvPtm0KdhIOUrz89LoqLR9e7TXAQAA0SB49UmlEkw5RtlWYnpa2rNHyuejuwYAAIgOwatPzIJKVJTTjbWatHFjdJ8PAACiRfDqo61bo28rQZd6AAAGF8Grj6LcqHp+XhobC+5oBAAAg4ng1UelUjAVGEVbidnZoFErAAAYXASvPtu5M5p1XgsL0ubN/f9cAAAQH4JXn0XZVoL1XQAADDaCV5/V12H1s63E4qI0PBw8AADA4CJ49VkUbSVmZqRt2/r3eQAAIBkErwhs2xasyeqXhQVpYqJ/nwcAAJJB8IrA+Ljk3t/PZH0XAACDj+AVgaGhoK3E3Nz6P6taDTbEHhlZ/2cBAIBkEbwismNHsLfies3OSpOTwdoxAAAw2AheEdm8uT/TjXNzNE4FACArCF4RqVT601bCPVgzBgAABl8iwcvMXmRm3zazB83sLUmMIWq5nLR7t3T2bO+fUasFnzM62r9xAQCA5MQevMwsL+k9kl4s6SpJrzazq+IeRxy2b19fxWtuLpiyzFGXBAAgE5L4J/16SQ+6+8PuviDp7yS9PIFxRG58XCqXe+/pNTMjbd3a3zEBAIDkJBG8dkk61PD6kfBY5phJ+/b1Pt1Yq7G+CwCALEkieLVqjLDi/j8ze4OZ3Wlmd544cSKGYUVjy5beNs12D4IbjVMBAMiOJILXI5IuaXi9W9KR5pPc/b3ufp27X7dlgPspjI4GVau1NlOdnw9+r1CIZlwAACB+SQSvr0q6wsyeYGYlSa+S9IkExhGbvXulqam1/Q4bYwMAkD2x11PcfcnMfl7SP0vKS3q/u98X9zjiNDkZ/KxPH3ajWg22HQIAANmRyESWu98s6eYkrp2E4eEgfJ07t7Y1W6zvAgAgW+gQFZM9e4Lpw24sLASbYg8NRTsmAAAQL4JXTDZtChqhdnOH48wM+zMCAJBFBK+YFIvSjh3S9PTq5y4uLq8LAwAA2UHwitGuXd23lWB9FwAA2UPwitHGjUFfrk77Ny4tSaVSsCAfAABkC8ErRvm8tHt3555e9fVd3badAAAAg4PgFbOdO4M1XO3Mz7OwHgCArCJ4xWx8PJhGbBe+3FnfBQBAVhG8YmYWbCF09uzK96rVYDpyZCT+cQEAgOgRvBKwdWsQsprNzgZtJHJ8KwAAZBL/xCegUpHGxla2lpibY30XAABZRvBKyN69K+9udA/WgAEAgGwieCVkcjIIWu7Ba/dg/RcL6wEAyC6CV0LKZWliYnnj7Lm5oMFqPp/osAAAQIQIXgnas2c5eM3MBIvuAQBAdhG8ErR5c/CzVgseGzcmOhwAABAxgleCSiVp+3Zpejp4zfouAACyjeCVsN27g2aqY2NSsZj0aAAAQJQIXgnbtCnYQoj+XQAAZF8h6QFc7PJ56bLLgjscAQBAthG8UuDyy5MeAQAAiANTjQAAADEheAEAAMSE4AUAABATghcAAEBMCF4AAAAxIXgBAADEhOAFAAAQE4IXAABATAheAAAAMSF4AQAAxITgBQAAEBOCFwAAQEwIXgAAADEheAEAAMSE4AUAABATghcAAEBMCF4AAAAxIXgBAADExNw96TGsysxOSDoQ8WUmJZ2M+BroHd9PevHdpBvfT3rx3aTber6fve6+pdUbAxG84mBmd7r7dUmPA63x/aQX30268f2kF99NukX1/TDVCAAAEBOCFwAAQEwIXsvem/QA0BHfT3rx3aQb30968d2kWyTfD2u8AAAAYkLFCwAAICYEL0lm9iIz+7aZPWhmb0l6PBc7M3u/mR03s3sbjm02s8+Y2XfCn5uSHOPFyswuMbPPmdkDZnafmb0xPM73kzAzGzazO8zsG+F38zvhcb6blDCzvJl93cz+d/ia7yYlzGy/mX3TzO42szvDY5F8Pxd98DKzvKT3SHqxpKskvdrMrkp2VBe9D0h6UdOxt0i6xd2vkHRL+BrxW5L0S+7+JEk3Svq58L8Xvp/kzUt6nrs/VdK1kl5kZjeK7yZN3ijpgYbXfDfp8lx3v7ahhUQk389FH7wkXS/pQXd/2N0XJP2dpJcnPKaLmrt/QdLjTYdfLumD4fMPSnpFnGNCwN2PuvvXwudTCv4R2SW+n8R5YDp8WQwfLr6bVDCz3ZJeKum/Nxzmu0m3SL4fglfwj8ahhtePhMeQLtvc/agU/OMvaWvC47nomdk+Sd8r6Xbx/aRCOJV1t6Tjkj7j7nw36fEuSW+WVGs4xneTHi7p02Z2l5m9ITwWyfdT6MeHDDhrcYxbPYEOzKwi6R8kvcndz5q1+s8IcXP3qqRrzWyjpI+Z2dUJDwmSzOyHJR1397vM7DkJDwetPcvdj5jZVkmfMbNvRXUhKl5BheuShte7JR1JaCxo75iZ7ZCk8OfxhMdz0TKzooLQ9WF3/2h4mO8nRdz9tKTPK1gryXeTvGdJepmZ7VewnOV5ZvY34rtJDXc/Ev48LuljCpYhRfL9ELykr0q6wsyeYGYlSa+S9ImEx4SVPiHppvD5TZI+nuBYLloWlLbeJ+kBd39nw1t8Pwkzsy1hpUtmVpb0fEnfEt9N4tz919x9t7vvU/BvzGfd/SfFd5MKZjZqZmP155J+SNK9iuj7oYGqJDN7iYL597yk97v77yU7ooubmX1E0nMU7Ax/TNJvS/pHSX8vaY+kg5J+zN2bF+AjYmb2/ZK+KOmbWl6r8usK1nnx/STIzJ6iYAFwXsH/VP+9u7/dzCbEd5Ma4VTjL7v7D/PdpIOZXaqgyiUFS7D+1t1/L6rvh+AFAAAQE6YaAQAAYkLwAgAAiAnBCwAAICYELwAAgJgQvAAAAGJC8AIwkMysamZ3Nzz6tsGwme0zs3v79XkAUMeWQQAG1ay7X5v0IABgLah4AcgUM9tvZn9kZneEj8vD43vN7BYzuyf8uSc8vs3MPmZm3wgf3xd+VN7M/tLM7jOzT4fd4GVmv2Bm94ef83cJ/TEBDCiCF4BBVW6aanxlw3tn3f16SX+uYFcKhc//2t2fIunDkt4dHn+3pFvd/amSnibpvvD4FZLe4+5PlnRa0o+Gx98i6XvDz/mZaP5oALKKzvUABpKZTbt7pcXx/ZKe5+4Phxt6P+ruE2Z2UtIOd18Mjx9190kzOyFpt7vPN3zGPkmfcfcrwte/Kqno7r9rZp+SNK1gG6t/dPfpiP+oADKEiheALPI2z9ud08p8w/OqltfEvlTSeyQ9XdJdZsZaWQBdI3gByKJXNvy8LXz+r5JeFT5/jaQvhc9vkfSzkmRmeTMbb/ehZpaTdIm7f07SmyVtlLSi6gYA7fB/agAGVdnM7m54/Sl3r7eUGDKz2xX8z+Wrw2O/IOn9ZvYrkk5I+qnw+BslvdfMXq+gsvWzko62uWZe0t+Y2QZJJulP3f10n/48AC4CrPECkCnhGq/r3P1k0mMBgGZMNQIAAMSEihcAAEBMqHgBAADEhOAFAAAQE4IXAABATAheAAAAMSF4AQAAxITgBQAAEJP/H8Af6EH5CPqXAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(mean)\n",
    "\n",
    "plt.fill_between(np.arange(0,50,1), ci[0], ci[1], color='blue', alpha=0.2)\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Mean Squared Loss(MSE)\")\n",
    "plt.title(\"MSE for test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1000, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.2223, -0.2197, -0.2295], device='cuda:0',\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "SelfAttentionModel = TimeSeriesModel(18,2,5000,1)\n",
    "SelfAttentionModel.to(device)\n",
    "dataiter = iter(dataloaders[\"train\"])\n",
    "sample = next(dataiter)\n",
    "print(sample[0].size())\n",
    "SelfAttentionModel(sample[0].to(device).float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1628.6304,   180.3035,   180.3035], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "tensor([3., 3., 7.])\n"
     ]
    }
   ],
   "source": [
    "sample[0].size()\n",
    "print(model(sample[0][:,-1000:,:].to(device)))\n",
    "print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = [1/num_experiment]*num_experiment  # This is for 5-fold cross-validation\n",
    "# split_ratio[-1] = 1 - sum(split_ratio[:-1])  # To ensure the ratios sum to 1\n",
    "split_ratio = [int(x*len(dataset)) for x in split_ratio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio[-1] = len(dataset) - sum(split_ratio[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 3, 3, 3, 3, 3, 3, 3, 7]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 26.4141\n",
      "test Loss: 33.6377\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 26.3859\n",
      "test Loss: 33.9996\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 26.3572\n",
      "test Loss: 34.0876\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 26.3280\n",
      "test Loss: 34.0836\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 26.2981\n",
      "test Loss: 34.0430\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 26.2676\n",
      "test Loss: 33.9864\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 26.2364\n",
      "test Loss: 33.9219\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 26.2145\n",
      "test Loss: 33.9167\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 26.2112\n",
      "test Loss: 33.9104\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 26.2080\n",
      "test Loss: 33.9036\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 26.2047\n",
      "test Loss: 33.8967\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 26.2015\n",
      "test Loss: 33.8897\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 26.1982\n",
      "test Loss: 33.8826\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 26.1949\n",
      "test Loss: 33.8756\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 26.1927\n",
      "test Loss: 33.8749\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 26.1924\n",
      "test Loss: 33.8742\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 26.1920\n",
      "test Loss: 33.8735\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 26.1917\n",
      "test Loss: 33.8728\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 26.1914\n",
      "test Loss: 33.8721\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 26.1911\n",
      "test Loss: 33.8714\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 26.1907\n",
      "test Loss: 33.8707\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 26.1905\n",
      "test Loss: 33.8707\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 26.1905\n",
      "test Loss: 33.8706\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 26.1904\n",
      "test Loss: 33.8705\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 26.1904\n",
      "test Loss: 33.8705\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 26.1904\n",
      "test Loss: 33.8704\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8703\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8703\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 26.1903\n",
      "test Loss: 33.8702\n",
      "Training complete in 0m 2s\n",
      "Best val Loss: 33.637737\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 24.2270\n",
      "test Loss: 35.8776\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 24.1923\n",
      "test Loss: 35.1576\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 24.1572\n",
      "test Loss: 34.8395\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 24.1216\n",
      "test Loss: 34.6612\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 24.0854\n",
      "test Loss: 34.5372\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 24.0485\n",
      "test Loss: 34.4347\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 24.0110\n",
      "test Loss: 34.3399\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 23.9852\n",
      "test Loss: 34.3276\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 23.9813\n",
      "test Loss: 34.3172\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 23.9774\n",
      "test Loss: 34.3076\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 23.9736\n",
      "test Loss: 34.2983\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 23.9697\n",
      "test Loss: 34.2893\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 23.9658\n",
      "test Loss: 34.2803\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 23.9619\n",
      "test Loss: 34.2714\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 23.9593\n",
      "test Loss: 34.2705\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 23.9589\n",
      "test Loss: 34.2696\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 23.9585\n",
      "test Loss: 34.2687\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 23.9581\n",
      "test Loss: 34.2678\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 23.9577\n",
      "test Loss: 34.2670\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 23.9574\n",
      "test Loss: 34.2661\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 23.9570\n",
      "test Loss: 34.2653\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 23.9567\n",
      "test Loss: 34.2652\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 23.9567\n",
      "test Loss: 34.2651\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 23.9566\n",
      "test Loss: 34.2650\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 23.9566\n",
      "test Loss: 34.2649\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 23.9566\n",
      "test Loss: 34.2648\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 23.9565\n",
      "test Loss: 34.2647\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 23.9565\n",
      "test Loss: 34.2647\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2647\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 23.9564\n",
      "test Loss: 34.2646\n",
      "Training complete in 0m 2s\n",
      "Best val Loss: 34.264599\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 33.9221\n",
      "test Loss: 17.7493\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 33.8726\n",
      "test Loss: 17.6651\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 33.8228\n",
      "test Loss: 17.6074\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 33.7724\n",
      "test Loss: 17.5488\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 33.7214\n",
      "test Loss: 17.4877\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 33.6695\n",
      "test Loss: 17.4244\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 33.6169\n",
      "test Loss: 17.3593\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 33.5801\n",
      "test Loss: 17.3527\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 33.5748\n",
      "test Loss: 17.3460\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 33.5694\n",
      "test Loss: 17.3393\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 33.5640\n",
      "test Loss: 17.3326\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 33.5585\n",
      "test Loss: 17.3260\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 33.5531\n",
      "test Loss: 17.3193\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 33.5477\n",
      "test Loss: 17.3127\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 33.5440\n",
      "test Loss: 17.3120\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 33.5434\n",
      "test Loss: 17.3113\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 33.5429\n",
      "test Loss: 17.3107\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 33.5423\n",
      "test Loss: 17.3100\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 33.5418\n",
      "test Loss: 17.3094\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 33.5412\n",
      "test Loss: 17.3087\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 33.5407\n",
      "test Loss: 17.3081\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 33.5403\n",
      "test Loss: 17.3080\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 33.5403\n",
      "test Loss: 17.3080\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 33.5402\n",
      "test Loss: 17.3079\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 33.5402\n",
      "test Loss: 17.3078\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 33.5401\n",
      "test Loss: 17.3078\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 33.5401\n",
      "test Loss: 17.3077\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 33.5400\n",
      "test Loss: 17.3076\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 33.5400\n",
      "test Loss: 17.3076\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 33.5400\n",
      "test Loss: 17.3076\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 33.5400\n",
      "test Loss: 17.3076\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 33.5399\n",
      "test Loss: 17.3076\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 33.5399\n",
      "test Loss: 17.3076\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 33.5399\n",
      "test Loss: 17.3076\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 33.5399\n",
      "test Loss: 17.3076\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 33.5399\n",
      "test Loss: 17.3076\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 33.5399\n",
      "test Loss: 17.3076\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 33.5399\n",
      "test Loss: 17.3076\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 33.5399\n",
      "test Loss: 17.3076\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 33.5399\n",
      "test Loss: 17.3076\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 33.5399\n",
      "test Loss: 17.3076\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 33.5399\n",
      "test Loss: 17.3076\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 33.5399\n",
      "test Loss: 17.3076\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 33.5399\n",
      "test Loss: 17.3076\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 33.5399\n",
      "test Loss: 17.3076\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 33.5399\n",
      "test Loss: 17.3076\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 33.5399\n",
      "test Loss: 17.3076\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 33.5399\n",
      "test Loss: 17.3076\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 33.5399\n",
      "test Loss: 17.3076\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 33.5399\n",
      "test Loss: 17.3076\n",
      "Training complete in 0m 2s\n",
      "Best val Loss: 17.307598\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 27.0683\n",
      "test Loss: 42.7165\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 27.0306\n",
      "test Loss: 43.6226\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 26.9926\n",
      "test Loss: 43.9415\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 26.9541\n",
      "test Loss: 44.0510\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 26.9150\n",
      "test Loss: 44.0763\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 26.8752\n",
      "test Loss: 44.0663\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 26.8347\n",
      "test Loss: 44.0412\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 26.8058\n",
      "test Loss: 44.0425\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 26.8017\n",
      "test Loss: 44.0410\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 26.7975\n",
      "test Loss: 44.0382\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 26.7933\n",
      "test Loss: 44.0350\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 26.7891\n",
      "test Loss: 44.0314\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 26.7849\n",
      "test Loss: 44.0278\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 26.7807\n",
      "test Loss: 44.0241\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 26.7778\n",
      "test Loss: 44.0237\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 26.7774\n",
      "test Loss: 44.0233\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 26.7770\n",
      "test Loss: 44.0229\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 26.7765\n",
      "test Loss: 44.0226\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 26.7761\n",
      "test Loss: 44.0222\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 26.7757\n",
      "test Loss: 44.0218\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 26.7753\n",
      "test Loss: 44.0214\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 26.7750\n",
      "test Loss: 44.0214\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 26.7749\n",
      "test Loss: 44.0213\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 26.7749\n",
      "test Loss: 44.0213\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 26.7749\n",
      "test Loss: 44.0212\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 26.7748\n",
      "test Loss: 44.0212\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 26.7748\n",
      "test Loss: 44.0212\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 26.7747\n",
      "test Loss: 44.0211\n",
      "Training complete in 0m 2s\n",
      "Best val Loss: 42.716549\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 31.7437\n",
      "test Loss: 17.7561\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 31.7080\n",
      "test Loss: 18.2194\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 31.6718\n",
      "test Loss: 18.3844\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 31.6351\n",
      "test Loss: 18.4340\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 31.5976\n",
      "test Loss: 18.4353\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 31.5594\n",
      "test Loss: 18.4159\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 31.5204\n",
      "test Loss: 18.3871\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 31.4939\n",
      "test Loss: 18.3864\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 31.4899\n",
      "test Loss: 18.3840\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 31.4859\n",
      "test Loss: 18.3809\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 31.4818\n",
      "test Loss: 18.3775\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 31.4778\n",
      "test Loss: 18.3740\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 31.4737\n",
      "test Loss: 18.3704\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 31.4697\n",
      "test Loss: 18.3669\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 31.4670\n",
      "test Loss: 18.3665\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 31.4666\n",
      "test Loss: 18.3661\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 31.4662\n",
      "test Loss: 18.3658\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 31.4658\n",
      "test Loss: 18.3654\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 31.4654\n",
      "test Loss: 18.3651\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 31.4650\n",
      "test Loss: 18.3647\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 31.4645\n",
      "test Loss: 18.3644\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 31.4643\n",
      "test Loss: 18.3643\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 31.4642\n",
      "test Loss: 18.3643\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 31.4642\n",
      "test Loss: 18.3642\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 31.4642\n",
      "test Loss: 18.3642\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 31.4641\n",
      "test Loss: 18.3642\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 31.4641\n",
      "test Loss: 18.3641\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 31.4640\n",
      "test Loss: 18.3641\n",
      "Training complete in 0m 2s\n",
      "Best val Loss: 17.756058\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 28.8887\n",
      "test Loss: 44.9811\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 28.8389\n",
      "test Loss: 44.9723\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 28.7885\n",
      "test Loss: 44.9345\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 28.7371\n",
      "test Loss: 44.8812\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 28.6847\n",
      "test Loss: 44.8212\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 28.6311\n",
      "test Loss: 44.7577\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 28.5762\n",
      "test Loss: 44.6920\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 28.5391\n",
      "test Loss: 44.6854\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 28.5334\n",
      "test Loss: 44.6787\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 28.5277\n",
      "test Loss: 44.6720\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 28.5220\n",
      "test Loss: 44.6652\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 28.5163\n",
      "test Loss: 44.6584\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 28.5106\n",
      "test Loss: 44.6516\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 28.5049\n",
      "test Loss: 44.6448\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 28.5011\n",
      "test Loss: 44.6441\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 28.5005\n",
      "test Loss: 44.6434\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 28.5000\n",
      "test Loss: 44.6428\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 28.4994\n",
      "test Loss: 44.6421\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 28.4988\n",
      "test Loss: 44.6414\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 28.4983\n",
      "test Loss: 44.6407\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 28.4977\n",
      "test Loss: 44.6401\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 28.4973\n",
      "test Loss: 44.6400\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 28.4972\n",
      "test Loss: 44.6399\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 28.4972\n",
      "test Loss: 44.6399\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 28.4971\n",
      "test Loss: 44.6398\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 28.4971\n",
      "test Loss: 44.6397\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 28.4970\n",
      "test Loss: 44.6397\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 28.4970\n",
      "test Loss: 44.6396\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6396\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6396\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6396\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6396\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6396\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6395\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6395\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6395\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6395\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6395\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6395\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6395\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6395\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6395\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6395\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6395\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6395\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6395\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6395\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6395\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6395\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 28.4969\n",
      "test Loss: 44.6395\n",
      "Training complete in 0m 2s\n",
      "Best val Loss: 44.639538\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 30.9149\n",
      "test Loss: 33.9780\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 30.8705\n",
      "test Loss: 33.5043\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 30.8253\n",
      "test Loss: 33.3727\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 30.7791\n",
      "test Loss: 33.3512\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 30.7317\n",
      "test Loss: 33.3681\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 30.6832\n",
      "test Loss: 33.3993\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 30.6333\n",
      "test Loss: 33.4363\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 30.5994\n",
      "test Loss: 33.4396\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 30.5943\n",
      "test Loss: 33.4434\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 30.5891\n",
      "test Loss: 33.4473\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 30.5839\n",
      "test Loss: 33.4511\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 30.5786\n",
      "test Loss: 33.4549\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 30.5734\n",
      "test Loss: 33.4585\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 30.5682\n",
      "test Loss: 33.4621\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 30.5647\n",
      "test Loss: 33.4624\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 30.5642\n",
      "test Loss: 33.4628\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 30.5637\n",
      "test Loss: 33.4631\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 30.5632\n",
      "test Loss: 33.4634\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 30.5626\n",
      "test Loss: 33.4637\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 30.5621\n",
      "test Loss: 33.4640\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 30.5616\n",
      "test Loss: 33.4643\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 30.5613\n",
      "test Loss: 33.4644\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 30.5612\n",
      "test Loss: 33.4644\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 30.5612\n",
      "test Loss: 33.4644\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 30.5611\n",
      "test Loss: 33.4644\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 30.5611\n",
      "test Loss: 33.4645\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 30.5610\n",
      "test Loss: 33.4645\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 30.5610\n",
      "test Loss: 33.4645\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 30.5609\n",
      "test Loss: 33.4645\n",
      "Training complete in 0m 2s\n",
      "Best val Loss: 33.351162\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 28.3473\n",
      "test Loss: 24.0737\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 28.2997\n",
      "test Loss: 23.5187\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 28.2515\n",
      "test Loss: 23.2667\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 28.2024\n",
      "test Loss: 23.1089\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 28.1522\n",
      "test Loss: 22.9808\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 28.1009\n",
      "test Loss: 22.8607\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 28.0483\n",
      "test Loss: 22.7413\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 28.0126\n",
      "test Loss: 22.7286\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 28.0072\n",
      "test Loss: 22.7163\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 28.0018\n",
      "test Loss: 22.7042\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 27.9963\n",
      "test Loss: 22.6923\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 27.9908\n",
      "test Loss: 22.6804\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 27.9854\n",
      "test Loss: 22.6687\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 27.9799\n",
      "test Loss: 22.6571\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 27.9762\n",
      "test Loss: 22.6560\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 27.9757\n",
      "test Loss: 22.6548\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 27.9751\n",
      "test Loss: 22.6537\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 27.9746\n",
      "test Loss: 22.6525\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 27.9740\n",
      "test Loss: 22.6514\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 27.9735\n",
      "test Loss: 22.6503\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 27.9730\n",
      "test Loss: 22.6492\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 27.9726\n",
      "test Loss: 22.6491\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 27.9725\n",
      "test Loss: 22.6490\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 27.9725\n",
      "test Loss: 22.6489\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 27.9724\n",
      "test Loss: 22.6488\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 27.9724\n",
      "test Loss: 22.6486\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 27.9723\n",
      "test Loss: 22.6485\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 27.9723\n",
      "test Loss: 22.6484\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 27.9722\n",
      "test Loss: 22.6484\n",
      "Training complete in 0m 2s\n",
      "Best val Loss: 22.648357\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 26.9420\n",
      "test Loss: 38.9666\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 26.8944\n",
      "test Loss: 40.5967\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 26.8463\n",
      "test Loss: 41.1575\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 26.7973\n",
      "test Loss: 41.3815\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 26.7472\n",
      "test Loss: 41.4937\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 26.6960\n",
      "test Loss: 41.5695\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 26.6436\n",
      "test Loss: 41.6345\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 26.6080\n",
      "test Loss: 41.6433\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 26.6026\n",
      "test Loss: 41.6503\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 26.5972\n",
      "test Loss: 41.6566\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 26.5918\n",
      "test Loss: 41.6625\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 26.5863\n",
      "test Loss: 41.6681\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 26.5809\n",
      "test Loss: 41.6735\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 26.5754\n",
      "test Loss: 41.6788\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 26.5718\n",
      "test Loss: 41.6793\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 26.5712\n",
      "test Loss: 41.6798\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 26.5707\n",
      "test Loss: 41.6803\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 26.5701\n",
      "test Loss: 41.6808\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 26.5696\n",
      "test Loss: 41.6812\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 26.5691\n",
      "test Loss: 41.6817\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 26.5685\n",
      "test Loss: 41.6821\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 26.5682\n",
      "test Loss: 41.6821\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 26.5681\n",
      "test Loss: 41.6822\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 26.5680\n",
      "test Loss: 41.6822\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 26.5680\n",
      "test Loss: 41.6823\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 26.5679\n",
      "test Loss: 41.6823\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 26.5679\n",
      "test Loss: 41.6823\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 26.5678\n",
      "test Loss: 41.6824\n",
      "Training complete in 0m 2s\n",
      "Best val Loss: 38.966621\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 29.3825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mohammad\\anaconda3\\envs\\ML\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 21.1770\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 29.3361\n",
      "test Loss: 20.8476\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 29.2890\n",
      "test Loss: 20.7209\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 29.2410\n",
      "test Loss: 20.6572\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 29.1919\n",
      "test Loss: 20.6143\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 29.1415\n",
      "test Loss: 20.5780\n",
      "Epoch 6/49\n",
      "----------\n",
      "train Loss: 29.0899\n",
      "test Loss: 20.5433\n",
      "Epoch 7/49\n",
      "----------\n",
      "train Loss: 29.0549\n",
      "test Loss: 20.5395\n",
      "Epoch 8/49\n",
      "----------\n",
      "train Loss: 29.0496\n",
      "test Loss: 20.5359\n",
      "Epoch 9/49\n",
      "----------\n",
      "train Loss: 29.0442\n",
      "test Loss: 20.5324\n",
      "Epoch 10/49\n",
      "----------\n",
      "train Loss: 29.0388\n",
      "test Loss: 20.5289\n",
      "Epoch 11/49\n",
      "----------\n",
      "train Loss: 29.0334\n",
      "test Loss: 20.5254\n",
      "Epoch 12/49\n",
      "----------\n",
      "train Loss: 29.0281\n",
      "test Loss: 20.5219\n",
      "Epoch 13/49\n",
      "----------\n",
      "train Loss: 29.0227\n",
      "test Loss: 20.5184\n",
      "Epoch 14/49\n",
      "----------\n",
      "train Loss: 29.0191\n",
      "test Loss: 20.5181\n",
      "Epoch 15/49\n",
      "----------\n",
      "train Loss: 29.0185\n",
      "test Loss: 20.5177\n",
      "Epoch 16/49\n",
      "----------\n",
      "train Loss: 29.0180\n",
      "test Loss: 20.5174\n",
      "Epoch 17/49\n",
      "----------\n",
      "train Loss: 29.0175\n",
      "test Loss: 20.5170\n",
      "Epoch 18/49\n",
      "----------\n",
      "train Loss: 29.0169\n",
      "test Loss: 20.5167\n",
      "Epoch 19/49\n",
      "----------\n",
      "train Loss: 29.0164\n",
      "test Loss: 20.5163\n",
      "Epoch 20/49\n",
      "----------\n",
      "train Loss: 29.0159\n",
      "test Loss: 20.5160\n",
      "Epoch 21/49\n",
      "----------\n",
      "train Loss: 29.0155\n",
      "test Loss: 20.5159\n",
      "Epoch 22/49\n",
      "----------\n",
      "train Loss: 29.0155\n",
      "test Loss: 20.5159\n",
      "Epoch 23/49\n",
      "----------\n",
      "train Loss: 29.0154\n",
      "test Loss: 20.5159\n",
      "Epoch 24/49\n",
      "----------\n",
      "train Loss: 29.0153\n",
      "test Loss: 20.5158\n",
      "Epoch 25/49\n",
      "----------\n",
      "train Loss: 29.0153\n",
      "test Loss: 20.5158\n",
      "Epoch 26/49\n",
      "----------\n",
      "train Loss: 29.0152\n",
      "test Loss: 20.5157\n",
      "Epoch 27/49\n",
      "----------\n",
      "train Loss: 29.0152\n",
      "test Loss: 20.5157\n",
      "Epoch 28/49\n",
      "----------\n",
      "train Loss: 29.0152\n",
      "test Loss: 20.5157\n",
      "Epoch 29/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Epoch 30/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Epoch 31/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Epoch 32/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Epoch 33/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Epoch 34/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Epoch 35/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Epoch 36/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Epoch 37/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Epoch 38/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Epoch 39/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Epoch 40/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Epoch 41/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Epoch 42/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Epoch 43/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Epoch 44/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Epoch 45/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Epoch 46/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Epoch 47/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Epoch 48/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Epoch 49/49\n",
      "----------\n",
      "train Loss: 29.0151\n",
      "test Loss: 20.5157\n",
      "Training complete in 0m 2s\n",
      "Best val Loss: 20.515680\n"
     ]
    }
   ],
   "source": [
    "num_experiment = 10\n",
    "num_epochs = 50\n",
    "a = np.zeros(shape=(num_experiment,num_epochs))\n",
    "Bests = np.zeros(shape=(num_experiment,))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define the proportion for each split (here, 5 folds)\n",
    "split_ratio = [1/num_experiment]*num_experiment  # This is for 5-fold cross-validation\n",
    "# split_ratio[-1] = 1 - sum(split_ratio[:-1])  # To ensure the ratios sum to 1\n",
    "split_ratio = [int(x*len(dataset)) for x in split_ratio]\n",
    "split_ratio[-1] = len(dataset) - sum(split_ratio[:-1])\n",
    "\n",
    "# Create the splits\n",
    "folds = random_split(dataset, lengths=split_ratio)\n",
    "dataloaders = {}\n",
    "dataset_sizes ={}\n",
    "# Now we can use these folds in our cross-validation\n",
    "for fold in range(num_experiment):\n",
    "    # Use one fold for validation and the others for training\n",
    "    validation_data = folds[fold]\n",
    "    training_data = torch.utils.data.ConcatDataset([folds[i] for i in range(5) if i!=fold])\n",
    "\n",
    "    train_dataloader = DataLoader(training_data, batch_size=3)\n",
    "    val_dataloader = DataLoader(validation_data, batch_size=3)\n",
    " \n",
    "    dataloaders[\"train\"] = train_dataloader\n",
    "    dataset_sizes[\"train\"] = len(training_data)\n",
    "    dataloaders[\"test\"] = val_dataloader\n",
    "    dataset_sizes[\"test\"] = len(validation_data)\n",
    "\n",
    "    # Now, you can create your model and train it using these dataloaders, \n",
    "    # and then evaluate it on the validation dataloader.\n",
    "    SelfAttentionModel = TimeSeriesModel(18,2,5000,1)\n",
    "    SelfAttentionModel.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer_ft = torch.optim.Adam(SelfAttentionModel.parameters(),lr=0.0003)\n",
    "    # optimizer_ft =  torch.optim.SGD(SelfAttentionModel.parameters(), lr=0.003, momentum=0.9)\n",
    "    exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "    constrain = WeightClipper()\n",
    "    model,best_Loss,test_loss_epoch = train_model(SelfAttentionModel,criterion,exp_lr_scheduler,optimizer_ft,dataloaders,dataset_sizes,0.004,constrain,50,torch.inf)\n",
    "    a[fold] = np.array(test_loss_epoch)\n",
    "    Bests[fold] = best_Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22230d34550>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.mean(a,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44.01262665, 37.44750977, 34.61843109, 38.55976486, 52.48387909,\n",
       "       24.87714386, 19.5505867 ,  5.52481222, 34.0345192 ,  4.86464161])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4303313430338622"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(t['Quality score'].values[index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
